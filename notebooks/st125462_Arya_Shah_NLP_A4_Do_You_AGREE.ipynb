{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0776a4-fab7-41bb-896e-4774f883db5d",
   "metadata": {},
   "source": [
    "#  AT82.05 Artificial Intelligence: Natural Language Understanding (NLU)\n",
    "\n",
    "## A4: Do You AGREE\n",
    "\n",
    "### Name: Arya Shah\n",
    "### StudentID: st125462\n",
    "\n",
    "-----------\n",
    "\n",
    "In this assignment, I will explore training a pre-trained model like BERT from scratch, focusing on leveraging text embeddings to capture semantic similarity. Additionally, we will explore how to adapt the loss function for tasks like Natural Language Inference (NLI) to enhance the model’s ability to understand semantic relationships between texts\n",
    "\n",
    "You can find the GitHub Repository for the assignment here:\n",
    "- https://github.com/aryashah2k/NLP-NLU (Complete Web App)\n",
    "- https://github.com/aryashah2k/NLP-NLU/tree/main/notebooks (Assignment Notebooks)\n",
    "- https://github.com/aryashah2k/NLP-NLU/tree/main/reports (Assignment Reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ea6ef-8a5d-4f73-8c20-0aa9bbb5f972",
   "metadata": {},
   "source": [
    "# Task 1: Training BERT from Scratch ✅\n",
    "\n",
    "Based on Masked Language Model/BERT-update.ipynb, modify as follows: (2 points)\n",
    "1) Implement Bidirectional Encoder Representations from Transformers (BERT) from scratch, following the concepts learned in class. ✅\n",
    "2) Train the model on a suitable dataset. Ensure to source this dataset from reputable public databases  or repositories. It is imperative to give proper credit to the dataset source in your documentation.✅\n",
    "3) Save the trained model weights for later use in Task 2.✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42db8f-ea67-4b5d-ace1-a9970f9f7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset Source Attribution and Credits\n",
    "\n",
    "\"\"\"\n",
    "Dataset Documentation\n",
    "\n",
    "1. BookCorpus Dataset\n",
    "-------------------\n",
    "Description:\n",
    "    A large collection of free novel books written by unpublished authors.\n",
    "    Contains approximately 74M sentences and 1B words from 11,038 books\n",
    "\n",
    "Usage:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Load full dataset\n",
    "    dataset = load_dataset('bookcorpus')\n",
    "    \n",
    "    # Load specific splits\n",
    "    train_test = load_dataset('bookcorpus', split='train+test')\n",
    "    \n",
    "    # Load percentage of data\n",
    "    partial_data = load_dataset('bookcorpus', split='train[:10%]')[1]\n",
    "\n",
    "2. SNLI (Stanford Natural Language Inference) Dataset\n",
    "-------------------------------------------------\n",
    "Description:\n",
    "    A collection of 570k human-written English sentence pairs labeled for \n",
    "    balanced classification with entailment, contradiction, and neutral labels.\n",
    "\n",
    "Structure:\n",
    "    - Total Instances: 570,152\n",
    "    - Splits:\n",
    "        - Train: 550,152\n",
    "        - Validation: 10,000\n",
    "        - Test: 10,000\n",
    "\n",
    "Data Fields:\n",
    "    - premise: str  # Base statement for comparison\n",
    "    - hypothesis: str  # Statement to be evaluated against premise\n",
    "    - label: int  # 0: entailment, 1: neutral, 2: contradiction, -1: no consensus\n",
    "\n",
    "Average Token Counts:\n",
    "    - Premise: 14.1 tokens\n",
    "    - Hypothesis: 8.3 tokens\n",
    "\n",
    "Usage:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    dataset = load_dataset('stanfordnlp/snli')\n",
    "    \n",
    "    # Filter invalid labels\n",
    "    valid_data = dataset.filter(lambda x: x['label'] != -1)\n",
    "\n",
    "Note:\n",
    "    Each premise appears in only one split, though it may be used in \n",
    "    multiple examples with different hypotheses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18d62d21-7b61-44b8-b989-85226079053e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:42:15,492 - INFO - Selected GPU 1 with 11004.50MB free memory\n",
      "2025-02-17 16:42:15,494 - INFO - Using device: cuda:1\n",
      "2025-02-17 16:42:15,502 - INFO - Starting BERT training from scratch\n",
      "2025-02-17 16:42:15,503 - INFO - Loading BookCorpus dataset...\n",
      "2025-02-17 16:42:19,577 - INFO - Preprocessing text data...\n",
      "2025-02-17 16:42:20,363 - INFO - Creating vocabulary...\n",
      "2025-02-17 16:42:20,579 - INFO - Vocabulary size: 27092\n",
      "2025-02-17 16:42:20,990 - INFO - Model initialized with gradient checkpointing\n",
      "2025-02-17 16:42:20,992 - INFO - Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac21b5c1819460bb3e3ce6dec2da3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 16:45:13,474 - INFO - Epoch 1 completed. Average loss: 7.9539 (from 2344 valid batches)\n",
      "2025-02-17 16:45:13,590 - INFO - Model saved to model_checkpoints/bert_epoch_1_20250217_164513.pt\n",
      "2025-02-17 16:45:13,591 - INFO - Config saved to model_checkpoints/config_20250217_164513.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7261d722834a03b06e763d4c844ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:48:06,279 - INFO - Epoch 2 completed. Average loss: 6.3747 (from 2344 valid batches)\n",
      "2025-02-17 16:48:06,383 - INFO - Model saved to model_checkpoints/bert_epoch_2_20250217_164806.pt\n",
      "2025-02-17 16:48:06,383 - INFO - Config saved to model_checkpoints/config_20250217_164806.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac19b0c71a14b07addb5f7b5e0d5e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:50:58,879 - INFO - Epoch 3 completed. Average loss: 6.1192 (from 2344 valid batches)\n",
      "2025-02-17 16:50:58,974 - INFO - Model saved to model_checkpoints/bert_epoch_3_20250217_165058.pt\n",
      "2025-02-17 16:50:58,975 - INFO - Config saved to model_checkpoints/config_20250217_165058.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a07514da8b408f86d0b413b6a6b947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:53:50,245 - INFO - Epoch 4 completed. Average loss: 5.7795 (from 2344 valid batches)\n",
      "2025-02-17 16:53:50,346 - INFO - Model saved to model_checkpoints/bert_epoch_4_20250217_165350.pt\n",
      "2025-02-17 16:53:50,347 - INFO - Config saved to model_checkpoints/config_20250217_165350.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef02128dd284453b899491530ddbda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:56:41,997 - INFO - Epoch 5 completed. Average loss: 5.5129 (from 2344 valid batches)\n",
      "2025-02-17 16:56:42,096 - INFO - Model saved to model_checkpoints/bert_epoch_5_20250217_165641.pt\n",
      "2025-02-17 16:56:42,097 - INFO - Config saved to model_checkpoints/config_20250217_165641.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9986eefd4ece4523b2e59e26fe681093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:59:33,606 - INFO - Epoch 6 completed. Average loss: 5.3713 (from 2344 valid batches)\n",
      "2025-02-17 16:59:33,713 - INFO - Model saved to model_checkpoints/bert_epoch_6_20250217_165933.pt\n",
      "2025-02-17 16:59:33,713 - INFO - Config saved to model_checkpoints/config_20250217_165933.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bd83f3a9e248779004febed0527939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:02:25,037 - INFO - Epoch 7 completed. Average loss: 5.2726 (from 2344 valid batches)\n",
      "2025-02-17 17:02:25,143 - INFO - Model saved to model_checkpoints/bert_epoch_7_20250217_170225.pt\n",
      "2025-02-17 17:02:25,144 - INFO - Config saved to model_checkpoints/config_20250217_170225.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f5fd1ea3b746d39905af0d1564da20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:05:16,252 - INFO - Epoch 8 completed. Average loss: 5.2281 (from 2344 valid batches)\n",
      "2025-02-17 17:05:16,354 - INFO - Model saved to model_checkpoints/bert_epoch_8_20250217_170516.pt\n",
      "2025-02-17 17:05:16,355 - INFO - Config saved to model_checkpoints/config_20250217_170516.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b1402f4fa440d09151e6f4bb4da215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:08:07,241 - INFO - Epoch 9 completed. Average loss: 5.2048 (from 2344 valid batches)\n",
      "2025-02-17 17:08:07,347 - INFO - Model saved to model_checkpoints/bert_epoch_9_20250217_170807.pt\n",
      "2025-02-17 17:08:07,347 - INFO - Config saved to model_checkpoints/config_20250217_170807.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178faa1fa71b44b68970127324425efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:10:58,332 - INFO - Epoch 10 completed. Average loss: 5.1990 (from 2344 valid batches)\n",
      "2025-02-17 17:10:58,437 - INFO - Model saved to model_checkpoints/bert_epoch_10_20250217_171058.pt\n",
      "2025-02-17 17:10:58,438 - INFO - Config saved to model_checkpoints/config_20250217_171058.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293f3ed5e9b74b2c9f74d6de74338564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:13:49,673 - INFO - Epoch 11 completed. Average loss: 5.2018 (from 2344 valid batches)\n",
      "2025-02-17 17:13:49,778 - INFO - Model saved to model_checkpoints/bert_epoch_11_20250217_171349.pt\n",
      "2025-02-17 17:13:49,779 - INFO - Config saved to model_checkpoints/config_20250217_171349.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6ea0cf44b4400b883aec4b965c3d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:16:40,828 - INFO - Epoch 12 completed. Average loss: 5.1833 (from 2344 valid batches)\n",
      "2025-02-17 17:16:40,933 - INFO - Model saved to model_checkpoints/bert_epoch_12_20250217_171640.pt\n",
      "2025-02-17 17:16:40,934 - INFO - Config saved to model_checkpoints/config_20250217_171640.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db486f6b2974d2dba8947266742a634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:19:32,317 - INFO - Epoch 13 completed. Average loss: 5.1567 (from 2344 valid batches)\n",
      "2025-02-17 17:19:32,422 - INFO - Model saved to model_checkpoints/bert_epoch_13_20250217_171932.pt\n",
      "2025-02-17 17:19:32,423 - INFO - Config saved to model_checkpoints/config_20250217_171932.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb76fc3714a4f739b845a27c970a3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:22:23,889 - INFO - Epoch 14 completed. Average loss: 5.1155 (from 2344 valid batches)\n",
      "2025-02-17 17:22:23,993 - INFO - Model saved to model_checkpoints/bert_epoch_14_20250217_172223.pt\n",
      "2025-02-17 17:22:23,994 - INFO - Config saved to model_checkpoints/config_20250217_172223.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347e7a95464a4b649c1444e2cc2862a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:25:15,268 - INFO - Epoch 15 completed. Average loss: 5.0737 (from 2344 valid batches)\n",
      "2025-02-17 17:25:15,367 - INFO - Model saved to model_checkpoints/bert_epoch_15_20250217_172515.pt\n",
      "2025-02-17 17:25:15,368 - INFO - Config saved to model_checkpoints/config_20250217_172515.json\n",
      "2025-02-17 17:25:15,369 - INFO - Training completed!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('bert_training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_free_gpu():\n",
    "    \"\"\"Get the GPU with the most available memory.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "    # Get the number of GPUs\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    if n_gpus == 0:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "    # Find GPU with most free memory\n",
    "    max_free_memory = 0\n",
    "    selected_gpu = 0\n",
    "    \n",
    "    for gpu_id in range(n_gpus):\n",
    "        try:\n",
    "            # Get free memory for this GPU\n",
    "            free_memory = torch.cuda.get_device_properties(gpu_id).total_memory - torch.cuda.memory_allocated(gpu_id)\n",
    "            if free_memory > max_free_memory:\n",
    "                max_free_memory = free_memory\n",
    "                selected_gpu = gpu_id\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    device = torch.device(f'cuda:{selected_gpu}')\n",
    "    logger.info(f\"Selected GPU {selected_gpu} with {max_free_memory/1024/1024:.2f}MB free memory\")\n",
    "    return device\n",
    "\n",
    "# Set device and seeds for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = get_free_gpu()\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "class BertConfig:\n",
    "    def __init__(self):\n",
    "        # Model architecture\n",
    "        self.vocab_size = None  # Will be set after data loading\n",
    "        self.hidden_size = 256\n",
    "        self.num_hidden_layers = 6\n",
    "        self.num_attention_heads = 8\n",
    "        self.intermediate_size = 1024\n",
    "        \n",
    "        # Dropout and normalization\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        self.layer_norm_eps = 1e-12\n",
    "        \n",
    "        # Sequence parameters\n",
    "        self.max_position_embeddings = 128\n",
    "        self.max_len = 128\n",
    "        self.type_vocab_size = 2\n",
    "        self.pad_token_id = 0\n",
    "        \n",
    "        # Special tokens\n",
    "        self.mask_token_id = 3\n",
    "        self.cls_token_id = 1\n",
    "        self.sep_token_id = 2\n",
    "        self.pad_token_id = 0\n",
    "        \n",
    "        # Training parameters\n",
    "        self.learning_rate = 1e-4\n",
    "        self.batch_size = 64\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.weight_decay = 0.01\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.warmup_ratio = 0.1\n",
    "\n",
    "class BertLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        variance = (x - mean).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - mean) / torch.sqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "            \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Initialize with smaller values for stability\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.layer_norm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in [self.query, self.key, self.value]:\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        \n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "            \n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        \n",
    "        return context_layer\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertSelfAttention(config)\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm1 = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.LayerNorm2 = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.activation = F.gelu\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output = self.LayerNorm1(attention_output + hidden_states)\n",
    "        \n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        intermediate_output = self.activation(intermediate_output)\n",
    "        \n",
    "        layer_output = self.output(intermediate_output)\n",
    "        layer_output = self.dropout(layer_output)\n",
    "        layer_output = self.LayerNorm2(layer_output + attention_output)\n",
    "        \n",
    "        return layer_output\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.pooler_activation = nn.Tanh()\n",
    "        \n",
    "        # MLM head\n",
    "        self.mlm_head = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        \n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        self.gradient_checkpointing = False\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        \n",
    "        hidden_states = embedding_output\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs)\n",
    "                    return custom_forward\n",
    "                \n",
    "                hidden_states = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer),\n",
    "                    hidden_states,\n",
    "                    extended_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = layer(hidden_states, extended_attention_mask)\n",
    "            \n",
    "        # MLM loss calculation\n",
    "        if masked_lm_labels is not None:\n",
    "            prediction_scores = self.mlm_head(hidden_states)\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), \n",
    "                                    masked_lm_labels.view(-1))\n",
    "            return masked_lm_loss\n",
    "            \n",
    "        return hidden_states\n",
    "    \n",
    "    def enable_gradient_checkpointing(self):\n",
    "        self.gradient_checkpointing = True\n",
    "\n",
    "def pad_sequence(tokens, max_len, pad_token):\n",
    "    \"\"\"Pad or truncate a sequence to max_len.\"\"\"\n",
    "    if len(tokens) > max_len:\n",
    "        return tokens[:max_len]\n",
    "    return tokens + [pad_token] * (max_len - len(tokens))\n",
    "\n",
    "def prepare_batch(texts, word2idx, config):\n",
    "    \"\"\"Prepare a batch of texts for BERT training.\"\"\"\n",
    "    # Convert texts to token ids and pad\n",
    "    batch_input_ids = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        token_ids = [word2idx.get(word, word2idx['[UNK]']) for word in tokens]\n",
    "        # Add [CLS] at start and [SEP] at end\n",
    "        token_ids = [word2idx['[CLS]']] + token_ids + [word2idx['[SEP]']]\n",
    "        # Pad sequence\n",
    "        padded_ids = pad_sequence(token_ids, config.max_len, word2idx['[PAD]'])\n",
    "        batch_input_ids.append(padded_ids)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_ids = torch.tensor(batch_input_ids).to(device)\n",
    "    attention_mask = (input_ids != word2idx['[PAD]']).float()\n",
    "    \n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    logger.info(\"Loading BookCorpus dataset...\")\n",
    "    # Load only 100k samples as specified\n",
    "    dataset = load_dataset('bookcorpus', split='train[:150000]')\n",
    "    \n",
    "    logger.info(\"Preprocessing text data...\")\n",
    "    texts = dataset['text']\n",
    "    texts = [text.lower() for text in texts]\n",
    "    texts = [re.sub(r'[^\\w\\s]', '', text) for text in texts]\n",
    "    \n",
    "    # Create vocabulary\n",
    "    logger.info(\"Creating vocabulary...\")\n",
    "    word_set = set()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_set.update(words)\n",
    "    \n",
    "    # Add special tokens\n",
    "    vocab = ['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]'] + list(word_set)\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return texts, word2idx, vocab\n",
    "\n",
    "def save_model_and_config(model, config, epoch, loss, save_dir='model_checkpoints'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_path = os.path.join(save_dir, f'bert_epoch_{epoch}_{timestamp}.pt')\n",
    "    config_path = os.path.join(save_dir, f'config_{timestamp}.json')\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, model_path)\n",
    "    \n",
    "    # Save config\n",
    "    config_dict = {k: v for k, v in vars(config).items() if not k.startswith('__')}\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_dict, f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Model saved to {model_path}\")\n",
    "    logger.info(f\"Config saved to {config_path}\")\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Starting BERT training from scratch\")\n",
    "    \n",
    "    # Enable mixed precision training with proper initialization\n",
    "    scaler = torch.amp.GradScaler(enabled=True)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    texts, word2idx, vocab = load_and_preprocess_data()\n",
    "    \n",
    "    # Initialize config\n",
    "    config = BertConfig()\n",
    "    config.vocab_size = len(vocab)\n",
    "    logger.info(f\"Vocabulary size: {config.vocab_size}\")\n",
    "    \n",
    "    # Initialize model with gradient checkpointing\n",
    "    model = BertModel(config).to(device)\n",
    "    model.enable_gradient_checkpointing()\n",
    "    logger.info(\"Model initialized with gradient checkpointing\")\n",
    "    \n",
    "    # Initialize optimizer with weight decay and proper learning rate\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.01\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0\n",
    "        }\n",
    "    ]\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate, eps=1e-8)\n",
    "    \n",
    "    # Add learning rate scheduler with warmup\n",
    "    num_training_steps = len(texts) // (config.batch_size * config.gradient_accumulation_steps) * 10\n",
    "    num_warmup_steps = num_training_steps // 10\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=0.5,  \n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    logger.info(\"Starting training...\")\n",
    "    model.train()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(15):\n",
    "            total_loss = 0\n",
    "            valid_loss_count = 0\n",
    "            optimizer.zero_grad()  # Reset gradients at start of epoch\n",
    "            \n",
    "            progress_bar = tqdm(range(0, len(texts), config.batch_size), \n",
    "                              desc=f\"Epoch {epoch+1}\")\n",
    "            \n",
    "            for step, batch_start in enumerate(progress_bar):\n",
    "                batch_texts = texts[batch_start:batch_start + config.batch_size]\n",
    "                \n",
    "                # Prepare batch data with padding\n",
    "                input_ids, attention_mask = prepare_batch(batch_texts, word2idx, config)\n",
    "                \n",
    "                # Create masked tokens\n",
    "                masked_labels = input_ids.clone()\n",
    "                special_tokens = {word2idx['[PAD]'], word2idx['[CLS]'], word2idx['[SEP]']}\n",
    "                mask_candidates = torch.ones_like(input_ids, device=device).bool()\n",
    "                for special_token in special_tokens:\n",
    "                    mask_candidates &= (input_ids != special_token)\n",
    "                \n",
    "                # Apply masking with 15% probability to valid tokens\n",
    "                mask_prob = torch.full(input_ids.shape, 0.15, device=device)\n",
    "                mask = (torch.bernoulli(mask_prob).bool() & mask_candidates)\n",
    "                \n",
    "                masked_labels[~mask] = -1  # Only compute loss on masked tokens\n",
    "                input_ids[mask] = word2idx['[MASK]']\n",
    "                \n",
    "                try:\n",
    "                    # Mixed precision forward pass\n",
    "                    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                        loss = model(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            masked_lm_labels=masked_labels\n",
    "                        )\n",
    "                        \n",
    "                        # Scale loss for gradient accumulation\n",
    "                        loss = loss / config.gradient_accumulation_steps\n",
    "                        \n",
    "                        # Check if loss is valid\n",
    "                        if not torch.isfinite(loss):\n",
    "                            logger.warning(f\"Non-finite loss detected: {loss.item()}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Backward pass with gradient scaling\n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # Gradient accumulation\n",
    "                    if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                        # Clip gradients\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        \n",
    "                        # Optimizer step\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    # Update loss statistics\n",
    "                    loss_value = loss.item() * config.gradient_accumulation_steps\n",
    "                    if np.isfinite(loss_value):\n",
    "                        total_loss += loss_value\n",
    "                        valid_loss_count += 1\n",
    "                    \n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': loss_value,\n",
    "                        'lr': scheduler.get_last_lr()[0]\n",
    "                    })\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        logger.warning(f\"Out of memory in batch. Skipping batch and clearing cache.\")\n",
    "                        if hasattr(torch.cuda, 'empty_cache'):\n",
    "                            torch.cuda.empty_cache()\n",
    "                        optimizer.zero_grad()\n",
    "                        continue\n",
    "                    raise e\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if step % 100 == 0 and hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Calculate average loss only from valid losses\n",
    "            avg_loss = total_loss / valid_loss_count if valid_loss_count > 0 else float('nan')\n",
    "            logger.info(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f} (from {valid_loss_count} valid batches)\")\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            save_model_and_config(model, config, epoch+1, avg_loss)\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            logger.error(f\"GPU out of memory error: {e}\")\n",
    "            logger.info(\"Try reducing batch_size or model size further if this error persists\")\n",
    "            if hasattr(torch.cuda, 'empty_cache'):\n",
    "                torch.cuda.empty_cache()\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6200d-2522-42a6-8662-b4027afb1b33",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "[Training Logs can be found in the logs directory in the a4_do_you_agree/bert_training.log]\n",
    "\n",
    "## Hardware and Initialization\n",
    "- Training used GPU 1 with approximately 11GB of free memory\n",
    "- Device: CUDA-enabled GPU (cuda:1)\n",
    "\n",
    "## Dataset and Model Setup\n",
    "- Successfully loaded BookCorpus dataset\n",
    "- Vocabulary size: 27,092 tokens\n",
    "- Model initialized with gradient checkpointing enabled\n",
    "\n",
    "## Training Progress\n",
    "- Total epochs completed: 15\n",
    "- Valid batches per epoch: 2,344\n",
    "- Training duration: Approximately 43 minutes (16:42 to 17:25)\n",
    "\n",
    "## Loss Progression\n",
    "Key loss values across epochs:\n",
    "- Epoch 1: 7.9539\n",
    "- Epoch 5: 5.5129\n",
    "- Epoch 10: 5.1990\n",
    "- Epoch 15: 5.0737 (final)\n",
    "\n",
    "## Model Performance Analysis\n",
    "- Strong initial improvement: Loss dropped significantly from 7.95 to 6.37 between epochs 1-2\n",
    "- Steady convergence: Loss continued to decrease gradually\n",
    "- Final improvement: ~36% reduction in loss from start (7.95) to finish (5.07)\n",
    "\n",
    "## Technical Notes\n",
    "- Model checkpoints and configurations were saved after each epoch\n",
    "- Some widget display errors were logged but didn't affect training\n",
    "- Warning about torch.utils.checkpoint parameter usage was recorded\n",
    "\n",
    "The training completed successfully with a clear trend of decreasing loss, indicating effective model learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ddfce-b111-43db-b355-f149c6d6e402",
   "metadata": {},
   "source": [
    "# Task 2: Task 2. Sentence Embedding with Sentence BERT✅\n",
    "\n",
    "Implement trained BERT from task 1 with siamese network structures to derive semantically meaningful sentence embeddings that can be compared\n",
    "using cosine-similarity. (3 points)✅\n",
    "1) Use the SNLI 4 OR MNLI 5 datasets from Hugging Face, or any dataset related to classification tasks.✅\n",
    "2) Reproduce training the Sentence-BERT as described in the paper 6.✅\n",
    "3) Focus on the Classification Objective Function: (SoftmaxLoss)\n",
    "\n",
    "o = softmax 1W T · (u, v, |u − v|)2✅\n",
    "\n",
    "HINT : You can take a look how to implement Softmax loss in the file 04 - Huggingface/Appendix - Sentence Embedding/S-BERT.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e4fe3b-c3c1-410e-9192-1f998d5c53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:30:52,384 - INFO - Using device: cuda\n",
      "2025-02-17 17:30:52,391 - INFO - Loading SNLI and MNLI datasets...\n",
      "2025-02-17 17:31:08,866 - INFO - Loaded 800 training samples and 800 validation samples\n",
      "2025-02-17 17:31:08,868 - INFO - Loaded 800 training samples and 800 validation samples\n",
      "2025-02-17 17:31:09,210 - INFO - Tokenizer vocabulary size: 30522\n",
      "/tmp/ipykernel_1859902/3502560219.py:259: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')  # Load to CPU first\n",
      "2025-02-17 17:31:09,327 - INFO - Loaded compatible weights from model_checkpoints/bert_epoch_15_20250217_172515.pt\n",
      "2025-02-17 17:31:09,329 - INFO - Enabled gradient checkpointing\n",
      "2025-02-17 17:31:09,417 - INFO - Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa9fe19b8ff4310bef01fe339dd79dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:31:31,000 - INFO - Epoch 1 completed. Average loss: 1.1087\n",
      "2025-02-17 17:31:31,002 - INFO - Epoch 1 Cosine Similarities - Mean: 0.9317, Std: 0.0296\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3a31fb5288475fac92fef227c1f35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:31:34,696 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:31:34,697 - INFO - Entailment: Mean=0.9508, Std=0.0280\n",
      "2025-02-17 17:31:34,698 - INFO - Contradiction: Mean=0.9532, Std=0.0235\n",
      "2025-02-17 17:31:34,699 - INFO - Neutral: Mean=0.9504, Std=0.0254\n",
      "2025-02-17 17:31:34,700 - INFO - Validation metrics: (0.35375, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.35      0.56      0.43       273\n",
      "           1       0.36      0.47      0.41       273\n",
      "           2       0.50      0.00      0.01       246\n",
      "\n",
      "    accuracy                           0.35       800\n",
      "   macro avg       0.30      0.26      0.21       800\n",
      "weighted avg       0.40      0.35      0.29       800\n",
      "')\n",
      "2025-02-17 17:31:34,729 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:31:34,749 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:31:34,750 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:31:34,751 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf7e8e03b004af282870acbbb0b6975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:31:55,485 - INFO - Epoch 2 completed. Average loss: 1.1006\n",
      "2025-02-17 17:31:55,487 - INFO - Epoch 2 Cosine Similarities - Mean: 0.9296, Std: 0.0310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09a5bd5e99f494f906d5c2cc6f81621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:31:59,168 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:31:59,169 - INFO - Entailment: Mean=0.9464, Std=0.0309\n",
      "2025-02-17 17:31:59,170 - INFO - Contradiction: Mean=0.9493, Std=0.0261\n",
      "2025-02-17 17:31:59,171 - INFO - Neutral: Mean=0.9463, Std=0.0280\n",
      "2025-02-17 17:31:59,172 - INFO - Validation metrics: (0.3275, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.35      0.66      0.46       273\n",
      "           1       0.36      0.09      0.14       273\n",
      "           2       0.27      0.24      0.25       246\n",
      "\n",
      "    accuracy                           0.33       800\n",
      "   macro avg       0.24      0.25      0.21       800\n",
      "weighted avg       0.32      0.33      0.28       800\n",
      "')\n",
      "2025-02-17 17:31:59,657 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:31:59,700 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:31:59,702 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:31:59,703 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7432642e60584736954c2cbfd57ba081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:32:20,828 - INFO - Epoch 3 completed. Average loss: 1.0883\n",
      "2025-02-17 17:32:20,830 - INFO - Epoch 3 Cosine Similarities - Mean: 0.9236, Std: 0.0361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79c00eec4bb456a9ad5095c8175177a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:32:24,525 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:32:24,526 - INFO - Entailment: Mean=0.9404, Std=0.0356\n",
      "2025-02-17 17:32:24,527 - INFO - Contradiction: Mean=0.9442, Std=0.0299\n",
      "2025-02-17 17:32:24,528 - INFO - Neutral: Mean=0.9409, Std=0.0320\n",
      "2025-02-17 17:32:24,529 - INFO - Validation metrics: (0.3475, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.42      0.19      0.27       273\n",
      "           1       0.36      0.67      0.47       273\n",
      "           2       0.25      0.17      0.21       246\n",
      "\n",
      "    accuracy                           0.35       800\n",
      "   macro avg       0.26      0.26      0.24       800\n",
      "weighted avg       0.34      0.35      0.31       800\n",
      "')\n",
      "2025-02-17 17:32:24,923 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:32:24,946 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:32:24,948 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:32:24,949 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b72d348c604c4487e99497ead8f306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:32:45,836 - INFO - Epoch 4 completed. Average loss: 1.0782\n",
      "2025-02-17 17:32:45,837 - INFO - Epoch 4 Cosine Similarities - Mean: 0.9149, Std: 0.0411\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5decb5339bce4855bb84e3c7e23fe4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:32:49,531 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:32:49,533 - INFO - Entailment: Mean=0.9309, Std=0.0434\n",
      "2025-02-17 17:32:49,533 - INFO - Contradiction: Mean=0.9364, Std=0.0364\n",
      "2025-02-17 17:32:49,535 - INFO - Neutral: Mean=0.9325, Std=0.0387\n",
      "2025-02-17 17:32:49,536 - INFO - Validation metrics: (0.3675, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.45      0.12      0.19       273\n",
      "           1       0.36      0.94      0.52       273\n",
      "           2       0.36      0.02      0.04       246\n",
      "\n",
      "    accuracy                           0.37       800\n",
      "   macro avg       0.29      0.27      0.19       800\n",
      "weighted avg       0.38      0.37      0.25       800\n",
      "')\n",
      "2025-02-17 17:32:50,085 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:32:50,123 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:32:50,125 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:32:50,126 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04da678409db449694e06f446c74595c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:33:10,826 - INFO - Epoch 5 completed. Average loss: 1.0695\n",
      "2025-02-17 17:33:10,828 - INFO - Epoch 5 Cosine Similarities - Mean: 0.9006, Std: 0.0498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d65a535d974badbd694fb969893183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:33:14,524 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:33:14,525 - INFO - Entailment: Mean=0.9173, Std=0.0536\n",
      "2025-02-17 17:33:14,527 - INFO - Contradiction: Mean=0.9243, Std=0.0455\n",
      "2025-02-17 17:33:14,527 - INFO - Neutral: Mean=0.9202, Std=0.0470\n",
      "2025-02-17 17:33:14,528 - INFO - Validation metrics: (0.38375, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.45      0.30      0.36       273\n",
      "           1       0.37      0.81      0.51       273\n",
      "           2       0.27      0.01      0.02       246\n",
      "\n",
      "    accuracy                           0.38       800\n",
      "   macro avg       0.27      0.28      0.22       800\n",
      "weighted avg       0.36      0.38      0.30       800\n",
      "')\n",
      "2025-02-17 17:33:14,972 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:33:15,013 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:33:15,014 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:33:15,016 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54e173e30be46758fc0f6bd8eff8331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:33:35,726 - INFO - Epoch 6 completed. Average loss: 1.0682\n",
      "2025-02-17 17:33:35,727 - INFO - Epoch 6 Cosine Similarities - Mean: 0.8822, Std: 0.0628\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5911f896ad7247fb85ebdaeecaac8f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:33:39,390 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:33:39,391 - INFO - Entailment: Mean=0.8948, Std=0.0722\n",
      "2025-02-17 17:33:39,391 - INFO - Contradiction: Mean=0.9052, Std=0.0614\n",
      "2025-02-17 17:33:39,392 - INFO - Neutral: Mean=0.9004, Std=0.0620\n",
      "2025-02-17 17:33:39,393 - INFO - Validation metrics: (0.39625, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.42      0.40      0.41       273\n",
      "           1       0.40      0.66      0.50       273\n",
      "           2       0.31      0.11      0.16       246\n",
      "\n",
      "    accuracy                           0.40       800\n",
      "   macro avg       0.28      0.29      0.27       800\n",
      "weighted avg       0.37      0.40      0.36       800\n",
      "')\n",
      "2025-02-17 17:33:39,730 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:33:39,769 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:33:39,771 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:33:39,772 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ea718677ee4db6a1ac80750fe0533d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:34:00,095 - INFO - Epoch 7 completed. Average loss: 1.0524\n",
      "2025-02-17 17:34:00,096 - INFO - Epoch 7 Cosine Similarities - Mean: 0.8602, Std: 0.0784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080706f0d6a244539e4e240af65dfdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:34:02,914 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:34:02,915 - INFO - Entailment: Mean=0.8690, Std=0.0957\n",
      "2025-02-17 17:34:02,917 - INFO - Contradiction: Mean=0.8843, Std=0.0812\n",
      "2025-02-17 17:34:02,917 - INFO - Neutral: Mean=0.8785, Std=0.0808\n",
      "2025-02-17 17:34:02,918 - INFO - Validation metrics: (0.3675, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.44      0.33      0.38       273\n",
      "           1       0.40      0.31      0.35       273\n",
      "           2       0.31      0.48      0.38       246\n",
      "\n",
      "    accuracy                           0.37       800\n",
      "   macro avg       0.29      0.28      0.28       800\n",
      "weighted avg       0.38      0.37      0.36       800\n",
      "')\n",
      "2025-02-17 17:34:03,304 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:34:03,342 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:34:03,344 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:34:03,345 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f55f32d890f4d0fa47d74026be6922d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:34:24,456 - INFO - Epoch 8 completed. Average loss: 1.0448\n",
      "2025-02-17 17:34:24,457 - INFO - Epoch 8 Cosine Similarities - Mean: 0.8389, Std: 0.0938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b5eff0b4834091b458e14211a6d19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:34:28,143 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:34:28,144 - INFO - Entailment: Mean=0.8479, Std=0.1113\n",
      "2025-02-17 17:34:28,145 - INFO - Contradiction: Mean=0.8646, Std=0.0956\n",
      "2025-02-17 17:34:28,146 - INFO - Neutral: Mean=0.8588, Std=0.0933\n",
      "2025-02-17 17:34:28,147 - INFO - Validation metrics: (0.4, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.43      0.44      0.43       273\n",
      "           1       0.42      0.48      0.45       273\n",
      "           2       0.33      0.28      0.31       246\n",
      "\n",
      "    accuracy                           0.40       800\n",
      "   macro avg       0.30      0.30      0.30       800\n",
      "weighted avg       0.39      0.40      0.39       800\n",
      "')\n",
      "2025-02-17 17:34:28,255 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:34:28,290 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:34:28,292 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:34:28,293 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eaf887dcbd94c36af92d684a64beaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:34:48,948 - INFO - Epoch 9 completed. Average loss: 1.0398\n",
      "2025-02-17 17:34:48,949 - INFO - Epoch 9 Cosine Similarities - Mean: 0.8044, Std: 0.1208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760b7d88bca44f82be2722cf4ea6d050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:34:51,855 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:34:51,856 - INFO - Entailment: Mean=0.8104, Std=0.1461\n",
      "2025-02-17 17:34:51,857 - INFO - Contradiction: Mean=0.8331, Std=0.1255\n",
      "2025-02-17 17:34:51,858 - INFO - Neutral: Mean=0.8260, Std=0.1206\n",
      "2025-02-17 17:34:51,858 - INFO - Validation metrics: (0.40375, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.43      0.45      0.44       273\n",
      "           1       0.43      0.51      0.47       273\n",
      "           2       0.32      0.25      0.28       246\n",
      "\n",
      "    accuracy                           0.40       800\n",
      "   macro avg       0.30      0.30      0.30       800\n",
      "weighted avg       0.39      0.40      0.40       800\n",
      "')\n",
      "2025-02-17 17:34:51,966 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:34:52,000 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:34:52,002 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:34:52,003 - INFO - Configuration saved to sbert_model/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273b1f4c1b4b439ba10d30676c900970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "2025-02-17 17:35:12,721 - INFO - Epoch 10 completed. Average loss: 1.0340\n",
      "2025-02-17 17:35:12,722 - INFO - Epoch 10 Cosine Similarities - Mean: 0.7670, Std: 0.1534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e677c0c18794c75af80ab8e5d6a9b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125462/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-02-17 17:35:16,390 - INFO - \n",
      "Cosine Similarity Analysis:\n",
      "2025-02-17 17:35:16,391 - INFO - Entailment: Mean=0.7780, Std=0.1795\n",
      "2025-02-17 17:35:16,392 - INFO - Contradiction: Mean=0.8076, Std=0.1534\n",
      "2025-02-17 17:35:16,393 - INFO - Neutral: Mean=0.7976, Std=0.1491\n",
      "2025-02-17 17:35:16,394 - INFO - Validation metrics: (0.4025, '              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         8\n",
      "           0       0.43      0.38      0.41       273\n",
      "           1       0.41      0.61      0.49       273\n",
      "           2       0.33      0.21      0.25       246\n",
      "\n",
      "    accuracy                           0.40       800\n",
      "   macro avg       0.29      0.30      0.29       800\n",
      "weighted avg       0.39      0.40      0.38       800\n",
      "')\n",
      "2025-02-17 17:35:16,481 - INFO - Model saved to sbert_model/model.pt\n",
      "2025-02-17 17:35:16,500 - INFO - Tokenizer saved to sbert_model/tokenizer\n",
      "2025-02-17 17:35:16,501 - INFO - Metrics saved to sbert_model/metrics.json\n",
      "2025-02-17 17:35:16,503 - INFO - Configuration saved to sbert_model/config.json\n",
      "2025-02-17 17:35:16,503 - INFO - Training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import BertTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('sbert_training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class SentenceBERT(nn.Module):\n",
    "    def __init__(self, bert_model=None, hidden_size=256, config=None):\n",
    "        super().__init__()\n",
    "        if bert_model is None:\n",
    "            from bert_scratch import BertModel, BertConfig\n",
    "            if config is None:\n",
    "                config = BertConfig()\n",
    "                config.hidden_size = hidden_size\n",
    "            self.bert = BertModel(config)\n",
    "        else:\n",
    "            self.bert = bert_model\n",
    "            \n",
    "        self.fc = nn.Linear(hidden_size * 3, 3)  # 3 classes: entailment, contradiction, neutral\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]  # Get hidden states if tuple is returned\n",
    "        embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, premise_input_ids, premise_attention_mask, \n",
    "               hypothesis_input_ids, hypothesis_attention_mask):\n",
    "        # Get embeddings for premise and hypothesis\n",
    "        premise_embedding = self.encode(premise_input_ids, premise_attention_mask)\n",
    "        hypothesis_embedding = self.encode(hypothesis_input_ids, hypothesis_attention_mask)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = F.cosine_similarity(premise_embedding, hypothesis_embedding)\n",
    "        self.last_cos_sim = cos_sim  # Store for later use\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        combined = torch.cat([\n",
    "            premise_embedding,\n",
    "            hypothesis_embedding,\n",
    "            torch.abs(premise_embedding - hypothesis_embedding)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Pass through classifier\n",
    "        logits = self.fc(combined)\n",
    "        \n",
    "        return logits, cos_sim  # Return both logits and cosine similarity\n",
    "\n",
    "def load_datasets(num_samples=None):\n",
    "    logger.info(\"Loading SNLI and MNLI datasets...\")\n",
    "    \n",
    "    # Load SNLI dataset\n",
    "    snli_dataset = load_dataset(\"snli\")\n",
    "    \n",
    "    # Load MNLI dataset\n",
    "    mnli_dataset = load_dataset(\"multi_nli\")\n",
    "    \n",
    "    # Rename MNLI labels to match SNLI\n",
    "    def rename_labels(example):\n",
    "        label_map = {0: 0, 1: 1, 2: 2}  # entailment: 0, contradiction: 1, neutral: 2\n",
    "        example['label'] = label_map[example['label']]\n",
    "        return example\n",
    "    \n",
    "    # Process MNLI dataset to match SNLI format\n",
    "    mnli_dataset = mnli_dataset.map(rename_labels)\n",
    "    \n",
    "    # Combine datasets\n",
    "    train_dataset = concatenate_datasets([\n",
    "        snli_dataset['train'],\n",
    "        mnli_dataset['train']\n",
    "    ])\n",
    "    \n",
    "    val_dataset = concatenate_datasets([\n",
    "        snli_dataset['validation'],\n",
    "        mnli_dataset['validation_matched']\n",
    "    ])\n",
    "    \n",
    "    # Subsample if specified\n",
    "    if num_samples is not None:\n",
    "        train_dataset = train_dataset.shuffle(seed=42).select(range(num_samples))\n",
    "        val_dataset = val_dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    \n",
    "    logger.info(f\"Loaded {len(train_dataset)} training samples and {len(val_dataset)} validation samples\")\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset\n",
    "    }\n",
    "\n",
    "def preprocess_data(datasets, tokenizer, max_length=128):\n",
    "    logger.info(\"Preprocessing datasets...\")\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize premises\n",
    "        premise_encodings = tokenizer(\n",
    "            examples['premise'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        # Tokenize hypotheses\n",
    "        hypothesis_encodings = tokenizer(\n",
    "            examples['hypothesis'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'premise_input_ids': premise_encodings['input_ids'],\n",
    "            'premise_attention_mask': premise_encodings['attention_mask'],\n",
    "            'hypothesis_input_ids': hypothesis_encodings['input_ids'],\n",
    "            'hypothesis_attention_mask': hypothesis_encodings['attention_mask'],\n",
    "            'labels': examples['label']\n",
    "        }\n",
    "    \n",
    "    # Process each split\n",
    "    processed_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        processed_datasets[split] = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        processed_datasets[split].set_format('torch')\n",
    "    \n",
    "    return processed_datasets\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_cosine_sims = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Get predictions\n",
    "            outputs, cos_sim = model(\n",
    "                batch['premise_input_ids'].to(device),\n",
    "                batch['premise_attention_mask'].to(device),\n",
    "                batch['hypothesis_input_ids'].to(device),\n",
    "                batch['hypothesis_attention_mask'].to(device)\n",
    "            )\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Collect predictions and labels\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].numpy())\n",
    "            all_cosine_sims.extend(cos_sim.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    # Calculate average cosine similarity for each class\n",
    "    cosine_sims = np.array(all_cosine_sims)\n",
    "    labels = np.array(all_labels)\n",
    "    \n",
    "    logger.info(\"\\nCosine Similarity Analysis:\")\n",
    "    for label in [0, 1, 2]:  # entailment, contradiction, neutral\n",
    "        mask = labels == label\n",
    "        if mask.any():\n",
    "            label_name = ['entailment', 'contradiction', 'neutral'][label]\n",
    "            sims = cosine_sims[mask]\n",
    "            logger.info(f\"{label_name.capitalize()}: Mean={sims.mean():.4f}, Std={sims.std():.4f}\")\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "def save_model(model, tokenizer, config, metrics, output_dir='sbert_model'):\n",
    "    \"\"\"Save the model, tokenizer, configuration and metrics.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state\n",
    "    model_path = os.path.join(output_dir, 'model.pt')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    logger.info(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_path = os.path.join(output_dir, 'tokenizer')\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "    logger.info(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = os.path.join(output_dir, 'metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    logger.info(f\"Metrics saved to {metrics_path}\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = os.path.join(output_dir, 'config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config.__dict__, f, indent=4)\n",
    "    logger.info(f\"Configuration saved to {config_path}\")\n",
    "\n",
    "def main():\n",
    "    # Load datasets with smaller batch size\n",
    "    datasets = load_datasets(num_samples=800)\n",
    "    logger.info(f\"Loaded {len(datasets['train'])} training samples and {len(datasets['validation'])} validation samples\")\n",
    "    \n",
    "    # Initialize tokenizer and get vocab size\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "    logger.info(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Load pre-trained BERT from our custom implementation\n",
    "    from bert_scratch import BertModel, BertConfig\n",
    "    config = BertConfig()\n",
    "    config.vocab_size = vocab_size  # Set vocab size to match tokenizer\n",
    "    config.batch_size = 2  # Extremely small batch size\n",
    "    config.hidden_size = 64  # Minimal hidden size\n",
    "    config.num_hidden_layers = 2  # Minimal number of layers\n",
    "    config.num_attention_heads = 4  # Reduced attention heads\n",
    "    config.gradient_accumulation_steps = 16  # Heavy gradient accumulation\n",
    "    config.intermediate_size = 256  # Minimal intermediate size\n",
    "    config.max_len = 32  # Minimal sequence length\n",
    "    config.attention_probs_dropout_prob = 0.1\n",
    "    config.hidden_dropout_prob = 0.1\n",
    "    \n",
    "    bert_model = BertModel(config)\n",
    "    \n",
    "    # Try to load pre-trained weights if available\n",
    "    try:\n",
    "        checkpoints = [f for f in os.listdir('model_checkpoints') if f.startswith('bert_epoch_')]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[2]))\n",
    "            checkpoint_path = os.path.join('model_checkpoints', latest_checkpoint)\n",
    "            \n",
    "            # Load checkpoint with proper handling\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')  # Load to CPU first\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['model_state_dict']\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "            \n",
    "            # Filter out mismatched keys\n",
    "            model_dict = bert_model.state_dict()\n",
    "            state_dict = {k: v for k, v in state_dict.items() \n",
    "                         if k in model_dict and v.shape == model_dict[k].shape}\n",
    "            \n",
    "            # Load filtered state dict\n",
    "            bert_model.load_state_dict(state_dict, strict=False)\n",
    "            logger.info(f\"Loaded compatible weights from {checkpoint_path}\")\n",
    "        else:\n",
    "            logger.warning(\"No pre-trained BERT checkpoints found. Starting with random initialization.\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to load pre-trained BERT weights: {e}\")\n",
    "    \n",
    "    # Initialize Sentence-BERT\n",
    "    model = SentenceBERT(bert_model, hidden_size=config.hidden_size, config=config)\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    if hasattr(model.bert, 'enable_gradient_checkpointing'):\n",
    "        model.bert.enable_gradient_checkpointing()\n",
    "        logger.info(\"Enabled gradient checkpointing\")\n",
    "    \n",
    "    # Move model to GPU after all initialization\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Clear GPU memory before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Preprocess datasets with reduced sequence length\n",
    "    tokenized_datasets = preprocess_data(datasets, tokenizer, max_length=config.max_len)\n",
    "    \n",
    "    # Create dataloaders with minimal batch size\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets['train'],\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        tokenized_datasets['validation'],\n",
    "        batch_size=config.batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer with weight decay\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': config.weight_decay\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=config.learning_rate,\n",
    "        eps=config.adam_epsilon\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            epoch_cos_sims = []\n",
    "            \n",
    "            progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "            for step, batch in enumerate(progress_bar):\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs, cos_sim = model(\n",
    "                    batch['premise_input_ids'].to(device),\n",
    "                    batch['premise_attention_mask'].to(device),\n",
    "                    batch['hypothesis_input_ids'].to(device),\n",
    "                    batch['hypothesis_attention_mask'].to(device)\n",
    "                )\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, batch['labels'].to(device))\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Update metrics\n",
    "                total_loss += loss.item()\n",
    "                epoch_cos_sims.extend(cos_sim.detach().cpu().numpy())\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({'loss': total_loss / (step + 1)})\n",
    "                \n",
    "                # Cleanup\n",
    "                del outputs, loss, batch\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            epoch_cos_sims = np.array(epoch_cos_sims)\n",
    "            \n",
    "            # Print epoch summary\n",
    "            logger.info(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "            logger.info(f\"Epoch {epoch+1} Cosine Similarities - Mean: {epoch_cos_sims.mean():.4f}, Std: {epoch_cos_sims.std():.4f}\")\n",
    "            \n",
    "            # Evaluate\n",
    "            accuracy, report = evaluate_model(model, val_dataloader)\n",
    "            logger.info(f\"Validation metrics: ({accuracy}, '{report}')\")\n",
    "            \n",
    "            # Save model\n",
    "            metrics_dict = {\n",
    "                'accuracy': float(accuracy),\n",
    "                'loss': float(avg_loss),\n",
    "                'classification_report': report,\n",
    "                'cosine_similarity_mean': float(epoch_cos_sims.mean()),\n",
    "                'cosine_similarity_std': float(epoch_cos_sims.std())\n",
    "            }\n",
    "            save_model(model, tokenizer, config, metrics_dict, output_dir='sbert_model')\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        logger.info(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f93e0-1485-42f5-a572-706c949f7578",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "## BERT Pre-training Results\n",
    "- Training completed 15 epochs\n",
    "- Initial vocabulary size: 27,092 tokens\n",
    "- Training device: GPU 1 with 11GB free memory\n",
    "- Loss progression:\n",
    "  - Starting loss: 7.9539 (Epoch 1)\n",
    "  - Final loss: 5.0737 (Epoch 15)\n",
    "  - Overall loss reduction: ~36%\n",
    "\n",
    "## SBERT Fine-tuning Results\n",
    "- Dataset: Combined SNLI and MNLI (800 training + 800 validation samples)\n",
    "- Model configuration:\n",
    "  - Vocabulary size: 30,522\n",
    "  - Hidden size: 64\n",
    "  - Layers: 2\n",
    "  - Attention heads: 4\n",
    "\n",
    "### Final Performance Metrics (After 10 epochs)\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|--------|\n",
    "| Accuracy | 40.25% |\n",
    "| Average Loss | 1.0340 |\n",
    "| Cosine Similarity Mean | 0.7670 |\n",
    "| Cosine Similarity Std | 0.1534 |\n",
    "\n",
    "### Class-wise Performance\n",
    "| Class | Precision | Recall | F1-Score |\n",
    "|-------|-----------|--------|-----------|\n",
    "| Entailment | 0.43 | 0.38 | 0.41 |\n",
    "| Contradiction | 0.41 | 0.61 | 0.49 |\n",
    "| Neutral | 0.33 | 0.21 | 0.25 |\n",
    "\n",
    "### Training Progression\n",
    "- Started with high cosine similarities (~0.93)\n",
    "- Gradually decreased to more discriminative values (~0.77)\n",
    "- Model showed steady improvement in classification performance\n",
    "- Final weighted average metrics:\n",
    "  - Precision: 0.39\n",
    "  - Recall: 0.40\n",
    "  - F1-score: 0.38\n",
    "\n",
    "Both training sessions completed successfully with proper model and checkpoint saving at each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578fd45-14ca-4844-ae09-30816257095b",
   "metadata": {},
   "source": [
    "# Task 3. Evaluation and Analysis (1 points) ✅\n",
    "\n",
    "1) Provide the performance metrics based on the SNLI or MNLI datasets for the Natural Language\n",
    "Inference (NLI) task. ✅\n",
    "\n",
    "2) Discuss any limitations or challenges encountered during the implementation and propose potential\n",
    "improvements or modifications. ✅\n",
    "\n",
    "\n",
    "NOTE: Make sure to provide proper documentation, including details of the datasets used, hyperparameters, and any modifications made to the original models\n",
    "\n",
    "----------------\n",
    "\n",
    "## 1. Performance Metrics\n",
    "\n",
    "### Table 1. Performance Table (SNLI + MNLI Combined Dataset)\n",
    "| Metric | Value |\n",
    "|--------|--------|\n",
    "| Overall Accuracy | 40.25% |\n",
    "| Average Loss | 1.0340 |\n",
    "| Macro Avg F1-score | 0.29 |\n",
    "| Weighted Avg F1-score | 0.38 |\n",
    "\n",
    "### Class-wise Performance\n",
    "| Class | Precision | Recall | F1-Score |\n",
    "|-------|-----------|--------|-----------|\n",
    "| Entailment | 0.43 | 0.38 | 0.41 |\n",
    "| Contradiction | 0.41 | 0.61 | 0.49 |\n",
    "| Neutral | 0.33 | 0.21 | 0.25 |\n",
    "\n",
    "### Cosine Similarity Analysis (Final Epoch)\n",
    "| Class | Mean | Std |\n",
    "|-------|------|-----|\n",
    "| Entailment | 0.7780 | 0.1795 |\n",
    "| Contradiction | 0.8076 | 0.1534 |\n",
    "| Neutral | 0.7976 | 0.1491 |\n",
    "\n",
    "## 2. Implementation Details\n",
    "\n",
    "### Dataset Information\n",
    "- Combined SNLI and MNLI datasets\n",
    "- Training samples: 800\n",
    "- Validation samples: 800\n",
    "- Tokenizer vocabulary size: 30,522\n",
    "\n",
    "### Hyperparameters\n",
    "- Hidden size: 64\n",
    "- Number of layers: 2\n",
    "- Attention heads: 4\n",
    "- Batch size: 2\n",
    "- Sequence length: 32\n",
    "- Learning rate: Not explicitly stated in logs\n",
    "- Training epochs: 10\n",
    "- Gradient accumulation steps: 16\n",
    "\n",
    "## 3. Limitations and Challenges\n",
    "\n",
    "1. **Resource Constraints:**\n",
    "   - Had to use minimal model architecture due to memory limitations\n",
    "   - Required gradient checkpointing for memory efficiency\n",
    "   - Small batch size (2) needed to fit in memory\n",
    "\n",
    "2. **Performance Limitations:**\n",
    "   - Relatively low accuracy (40.25%)\n",
    "   - Poor performance on neutral class (F1: 0.25)\n",
    "   - High cosine similarities between different classes\n",
    "\n",
    "3. **Dataset Challenges:**\n",
    "   - Small training set (800 samples) may not be representative\n",
    "   - Imbalanced class distribution\n",
    "   - Some invalid labels present (-1 class with 8 samples)\n",
    "\n",
    "## 4. Proposed Improvements\n",
    "\n",
    "1. **Model Architecture:**\n",
    "   - Increase model capacity (more layers, wider hidden dimensions)\n",
    "   - Implement attention mechanisms specific to NLI tasks\n",
    "   - Add residual connections for better gradient flow\n",
    "\n",
    "2. **Training Strategy:**\n",
    "   - Use larger batch sizes with gradient accumulation\n",
    "   - Implement curriculum learning\n",
    "   - Add contrastive learning objectives\n",
    "\n",
    "3. **Data Processing:**\n",
    "   - Use larger training dataset\n",
    "   - Balance class distribution\n",
    "   - Better handling of invalid labels\n",
    "   - Implement data augmentation techniques\n",
    "\n",
    "4. **Optimization:**\n",
    "   - Try different learning rate schedules\n",
    "   - Implement early stopping\n",
    "   - Use mixed-precision training\n",
    "   - Add regularization techniques\n",
    "\n",
    "The implementation shows proof of concept but would benefit from these improvements for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac57e10-f40c-4955-82c1-744af4de62ba",
   "metadata": {},
   "source": [
    "# Task 4. Text similarity - Web Application Development ✅\n",
    "\n",
    "Develop a simple web application that demonstrates the capabilities of your text-embedding model. (1 points) ✅\n",
    "1) Develop a simple website with two input boxes for search queries.✅\n",
    "2) Utilize a custom-trained sentence transformer model to predict Natural Language Inference (NLI)✅\n",
    "\n",
    "Task (entailment, neutral and contradiction).\n",
    "For example:\n",
    "• Premise: A man is playing a guitar on stage.\n",
    "• Hypothesis: The man is performing music.\n",
    "• Label: Entailment\n",
    "\n",
    "The app is available on GitHub Lionks mentioned in the beginning of the notebook!\n",
    "\n",
    "Below is the inference script for the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e610c66-fcf3-407f-bdab-6adc1db11d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 08:43:49,069 - INFO - Using device: cuda\n",
      "2025-02-22 08:43:49,120 - INFO - Selected GPU 0 with 11004.50MB free memory\n",
      "2025-02-22 08:43:49,121 - INFO - Using device: cuda:0\n",
      "2025-02-22 08:43:49,123 - INFO - Using device: cuda\n",
      "2025-02-22 08:43:49,165 - INFO - Loaded tokenizer with vocabulary size: 30522\n",
      "/tmp/ipykernel_2166839/3675333752.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n",
      "2025-02-22 08:43:49,432 - INFO - Loaded model weights from sbert_model/model.pt\n",
      "2025-02-22 08:43:49,439 - INFO - Model initialized successfully\n",
      "2025-02-22 08:43:49,830 - INFO - \n",
      "Example 1:\n",
      "2025-02-22 08:43:49,831 - INFO - Premise: A man is playing a guitar on stage.\n",
      "2025-02-22 08:43:49,832 - INFO - Hypothesis: The man is performing music.\n",
      "2025-02-22 08:43:49,832 - INFO - Expected: entailment\n",
      "2025-02-22 08:43:49,833 - INFO - Predicted: contradiction\n",
      "2025-02-22 08:43:49,834 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,835 - INFO -   entailment: 0.301\n",
      "2025-02-22 08:43:49,835 - INFO -   contradiction: 0.389\n",
      "2025-02-22 08:43:49,836 - INFO -   neutral: 0.310\n",
      "2025-02-22 08:43:49,846 - INFO - \n",
      "Example 2:\n",
      "2025-02-22 08:43:49,847 - INFO - Premise: The cat is sleeping on the couch.\n",
      "2025-02-22 08:43:49,847 - INFO - Hypothesis: The dog is running in the park.\n",
      "2025-02-22 08:43:49,848 - INFO - Expected: contradiction\n",
      "2025-02-22 08:43:49,849 - INFO - Predicted: neutral\n",
      "2025-02-22 08:43:49,849 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,850 - INFO -   entailment: 0.216\n",
      "2025-02-22 08:43:49,851 - INFO -   contradiction: 0.388\n",
      "2025-02-22 08:43:49,851 - INFO -   neutral: 0.396\n",
      "2025-02-22 08:43:49,861 - INFO - \n",
      "Example 3:\n",
      "2025-02-22 08:43:49,861 - INFO - Premise: A woman is reading a book.\n",
      "2025-02-22 08:43:49,862 - INFO - Hypothesis: She is wearing glasses.\n",
      "2025-02-22 08:43:49,862 - INFO - Expected: neutral\n",
      "2025-02-22 08:43:49,863 - INFO - Predicted: entailment\n",
      "2025-02-22 08:43:49,864 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,865 - INFO -   entailment: 0.598\n",
      "2025-02-22 08:43:49,865 - INFO -   contradiction: 0.201\n",
      "2025-02-22 08:43:49,866 - INFO -   neutral: 0.201\n",
      "2025-02-22 08:43:49,875 - INFO - \n",
      "Example 4:\n",
      "2025-02-22 08:43:49,876 - INFO - Premise: Children are playing soccer in the park.\n",
      "2025-02-22 08:43:49,876 - INFO - Hypothesis: Kids are engaged in outdoor sports.\n",
      "2025-02-22 08:43:49,877 - INFO - Expected: entailment\n",
      "2025-02-22 08:43:49,878 - INFO - Predicted: entailment\n",
      "2025-02-22 08:43:49,879 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,880 - INFO -   entailment: 0.709\n",
      "2025-02-22 08:43:49,880 - INFO -   contradiction: 0.107\n",
      "2025-02-22 08:43:49,881 - INFO -   neutral: 0.184\n",
      "2025-02-22 08:43:49,891 - INFO - \n",
      "Example 5:\n",
      "2025-02-22 08:43:49,891 - INFO - Premise: The restaurant is packed with customers.\n",
      "2025-02-22 08:43:49,892 - INFO - Hypothesis: The restaurant is closed today.\n",
      "2025-02-22 08:43:49,892 - INFO - Expected: contradiction\n",
      "2025-02-22 08:43:49,893 - INFO - Predicted: contradiction\n",
      "2025-02-22 08:43:49,894 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,894 - INFO -   entailment: 0.282\n",
      "2025-02-22 08:43:49,895 - INFO -   contradiction: 0.386\n",
      "2025-02-22 08:43:49,896 - INFO -   neutral: 0.332\n",
      "2025-02-22 08:43:49,905 - INFO - \n",
      "Example 6:\n",
      "2025-02-22 08:43:49,906 - INFO - Premise: A student is writing notes in class.\n",
      "2025-02-22 08:43:49,906 - INFO - Hypothesis: The student understands the material.\n",
      "2025-02-22 08:43:49,907 - INFO - Expected: neutral\n",
      "2025-02-22 08:43:49,907 - INFO - Predicted: contradiction\n",
      "2025-02-22 08:43:49,908 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,909 - INFO -   entailment: 0.258\n",
      "2025-02-22 08:43:49,910 - INFO -   contradiction: 0.399\n",
      "2025-02-22 08:43:49,910 - INFO -   neutral: 0.343\n",
      "2025-02-22 08:43:49,919 - INFO - \n",
      "Example 7:\n",
      "2025-02-22 08:43:49,920 - INFO - Premise: The chef is preparing pasta in the kitchen.\n",
      "2025-02-22 08:43:49,920 - INFO - Hypothesis: Someone is cooking food.\n",
      "2025-02-22 08:43:49,921 - INFO - Expected: entailment\n",
      "2025-02-22 08:43:49,921 - INFO - Predicted: entailment\n",
      "2025-02-22 08:43:49,922 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,923 - INFO -   entailment: 0.868\n",
      "2025-02-22 08:43:49,924 - INFO -   contradiction: 0.046\n",
      "2025-02-22 08:43:49,924 - INFO -   neutral: 0.087\n",
      "2025-02-22 08:43:49,933 - INFO - \n",
      "Example 8:\n",
      "2025-02-22 08:43:49,934 - INFO - Premise: The sky is clear and blue today.\n",
      "2025-02-22 08:43:49,934 - INFO - Hypothesis: It is raining heavily.\n",
      "2025-02-22 08:43:49,935 - INFO - Expected: contradiction\n",
      "2025-02-22 08:43:49,936 - INFO - Predicted: entailment\n",
      "2025-02-22 08:43:49,936 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,937 - INFO -   entailment: 0.773\n",
      "2025-02-22 08:43:49,938 - INFO -   contradiction: 0.106\n",
      "2025-02-22 08:43:49,938 - INFO -   neutral: 0.121\n",
      "2025-02-22 08:43:49,947 - INFO - \n",
      "Example 9:\n",
      "2025-02-22 08:43:49,947 - INFO - Premise: A person bought a new laptop.\n",
      "2025-02-22 08:43:49,948 - INFO - Hypothesis: They got it from Amazon.\n",
      "2025-02-22 08:43:49,949 - INFO - Expected: neutral\n",
      "2025-02-22 08:43:49,949 - INFO - Predicted: contradiction\n",
      "2025-02-22 08:43:49,950 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,951 - INFO -   entailment: 0.291\n",
      "2025-02-22 08:43:49,951 - INFO -   contradiction: 0.398\n",
      "2025-02-22 08:43:49,952 - INFO -   neutral: 0.311\n",
      "2025-02-22 08:43:49,961 - INFO - \n",
      "Example 10:\n",
      "2025-02-22 08:43:49,961 - INFO - Premise: The train arrived 30 minutes late.\n",
      "2025-02-22 08:43:49,962 - INFO - Hypothesis: The train was delayed.\n",
      "2025-02-22 08:43:49,963 - INFO - Expected: entailment\n",
      "2025-02-22 08:43:49,963 - INFO - Predicted: entailment\n",
      "2025-02-22 08:43:49,964 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,965 - INFO -   entailment: 0.356\n",
      "2025-02-22 08:43:49,965 - INFO -   contradiction: 0.350\n",
      "2025-02-22 08:43:49,966 - INFO -   neutral: 0.294\n",
      "2025-02-22 08:43:49,975 - INFO - \n",
      "Example 11:\n",
      "2025-02-22 08:43:49,975 - INFO - Premise: The museum is open on weekends.\n",
      "2025-02-22 08:43:49,976 - INFO - Hypothesis: The museum is closed every day.\n",
      "2025-02-22 08:43:49,976 - INFO - Expected: contradiction\n",
      "2025-02-22 08:43:49,977 - INFO - Predicted: entailment\n",
      "2025-02-22 08:43:49,977 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,979 - INFO -   entailment: 0.375\n",
      "2025-02-22 08:43:49,979 - INFO -   contradiction: 0.358\n",
      "2025-02-22 08:43:49,980 - INFO -   neutral: 0.268\n",
      "2025-02-22 08:43:49,989 - INFO - \n",
      "Example 12:\n",
      "2025-02-22 08:43:49,989 - INFO - Premise: A woman is walking her dog.\n",
      "2025-02-22 08:43:49,990 - INFO - Hypothesis: The dog is brown in color.\n",
      "2025-02-22 08:43:49,990 - INFO - Expected: neutral\n",
      "2025-02-22 08:43:49,991 - INFO - Predicted: contradiction\n",
      "2025-02-22 08:43:49,991 - INFO - Probabilities:\n",
      "2025-02-22 08:43:49,992 - INFO -   entailment: 0.289\n",
      "2025-02-22 08:43:49,993 - INFO -   contradiction: 0.410\n",
      "2025-02-22 08:43:49,994 - INFO -   neutral: 0.300\n",
      "2025-02-22 08:43:50,002 - INFO - \n",
      "Example 13:\n",
      "2025-02-22 08:43:50,003 - INFO - Premise: The movie theater is showing new releases.\n",
      "2025-02-22 08:43:50,003 - INFO - Hypothesis: Films are being screened.\n",
      "2025-02-22 08:43:50,004 - INFO - Expected: entailment\n",
      "2025-02-22 08:43:50,005 - INFO - Predicted: entailment\n",
      "2025-02-22 08:43:50,005 - INFO - Probabilities:\n",
      "2025-02-22 08:43:50,006 - INFO -   entailment: 0.711\n",
      "2025-02-22 08:43:50,007 - INFO -   contradiction: 0.132\n",
      "2025-02-22 08:43:50,007 - INFO -   neutral: 0.157\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import BertTokenizer\n",
    "from sentence_bert import SentenceBERT\n",
    "from bert_scratch import BertConfig\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NLIPredictor:\n",
    "    def __init__(self, model_dir='sbert_model'):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize tokenizer first to get vocab size\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(f\"{model_dir}/tokenizer\")\n",
    "        logger.info(f\"Loaded tokenizer with vocabulary size: {len(self.tokenizer.vocab)}\")\n",
    "        \n",
    "        # Initialize BERT config with SBERT values\n",
    "        from bert_scratch import BertConfig\n",
    "        config = BertConfig()\n",
    "        \n",
    "        # Set model architecture parameters\n",
    "        config.vocab_size = 30522  # Fixed vocab size from SBERT config\n",
    "        config.hidden_size = 64\n",
    "        config.num_hidden_layers = 2\n",
    "        config.num_attention_heads = 4\n",
    "        config.intermediate_size = 256\n",
    "        config.max_position_embeddings = 128\n",
    "        config.max_len = 32\n",
    "        config.type_vocab_size = 2\n",
    "        \n",
    "        # Set dropout and normalization parameters\n",
    "        config.hidden_dropout_prob = 0.1\n",
    "        config.attention_probs_dropout_prob = 0.1\n",
    "        config.layer_norm_eps = 1e-12\n",
    "        \n",
    "        # Set special token IDs\n",
    "        config.pad_token_id = 0\n",
    "        config.mask_token_id = 3\n",
    "        config.cls_token_id = 1\n",
    "        config.sep_token_id = 2\n",
    "        \n",
    "        # Training parameters (required by BertConfig)\n",
    "        config.learning_rate = 1e-4\n",
    "        config.batch_size = 2\n",
    "        config.gradient_accumulation_steps = 16\n",
    "        config.weight_decay = 0.01\n",
    "        config.adam_epsilon = 1e-8\n",
    "        config.warmup_ratio = 0.1\n",
    "        \n",
    "        # Initialize SentenceBERT with the config\n",
    "        from sentence_bert import SentenceBERT\n",
    "        self.model = SentenceBERT(None, hidden_size=config.hidden_size, config=config)\n",
    "        \n",
    "        # Load model weights\n",
    "        try:\n",
    "            model_path = f\"{model_dir}/model.pt\"\n",
    "            state_dict = torch.load(model_path, map_location=self.device)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            logger.info(f\"Loaded model weights from {model_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model weights: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Label mapping\n",
    "        self.id2label = {0: 'entailment', 1: 'contradiction', 2: 'neutral'}\n",
    "        \n",
    "        logger.info(\"Model initialized successfully\")\n",
    "    \n",
    "    def predict(self, premise, hypothesis):\n",
    "        # Clear GPU cache before prediction\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(\n",
    "            [premise, hypothesis],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=32,  # Use fixed max_length from config\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Split inputs for premise and hypothesis\n",
    "        premise_input_ids = inputs['input_ids'][0].unsqueeze(0)\n",
    "        premise_attention_mask = inputs['attention_mask'][0].unsqueeze(0)\n",
    "        hypothesis_input_ids = inputs['input_ids'][1].unsqueeze(0)\n",
    "        hypothesis_attention_mask = inputs['attention_mask'][1].unsqueeze(0)\n",
    "        \n",
    "        # Move to device\n",
    "        premise_input_ids = premise_input_ids.to(self.device)\n",
    "        premise_attention_mask = premise_attention_mask.to(self.device)\n",
    "        hypothesis_input_ids = hypothesis_input_ids.to(self.device)\n",
    "        hypothesis_attention_mask = hypothesis_attention_mask.to(self.device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = self.model(\n",
    "                    premise_input_ids,\n",
    "                    premise_attention_mask,\n",
    "                    hypothesis_input_ids,\n",
    "                    hypothesis_attention_mask\n",
    "                )\n",
    "                prediction = torch.argmax(outputs, dim=1).item()\n",
    "                probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
    "        \n",
    "        result = {\n",
    "            'label': self.id2label[prediction],\n",
    "            'probabilities': {\n",
    "                self.id2label[i]: float(prob.item())\n",
    "                for i, prob in enumerate(probabilities)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Clear memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return result\n",
    "\n",
    "def main():\n",
    "    # Initialize predictor\n",
    "    predictor = NLIPredictor()\n",
    "    \n",
    "    # Example usage\n",
    "    examples = [\n",
    "        {\n",
    "            'premise': 'A man is playing a guitar on stage.',\n",
    "            'hypothesis': 'The man is performing music.',\n",
    "            'expected': 'entailment'\n",
    "        },\n",
    "        {\n",
    "            'premise': 'The cat is sleeping on the couch.',\n",
    "            'hypothesis': 'The dog is running in the park.',\n",
    "            'expected': 'contradiction'\n",
    "        },\n",
    "        {\n",
    "            'premise': 'A woman is reading a book.',\n",
    "            'hypothesis': 'She is wearing glasses.',\n",
    "            'expected': 'neutral'\n",
    "        },\n",
    "        {\n",
    "        'premise': 'Children are playing soccer in the park.',\n",
    "        'hypothesis': 'Kids are engaged in outdoor sports.',\n",
    "        'expected': 'entailment'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'The restaurant is packed with customers.',\n",
    "        'hypothesis': 'The restaurant is closed today.',\n",
    "        'expected': 'contradiction'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'A student is writing notes in class.',\n",
    "        'hypothesis': 'The student understands the material.',\n",
    "        'expected': 'neutral'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'The chef is preparing pasta in the kitchen.',\n",
    "        'hypothesis': 'Someone is cooking food.',\n",
    "        'expected': 'entailment'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'The sky is clear and blue today.',\n",
    "        'hypothesis': 'It is raining heavily.',\n",
    "        'expected': 'contradiction'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'A person bought a new laptop.',\n",
    "        'hypothesis': 'They got it from Amazon.',\n",
    "        'expected': 'neutral'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'The train arrived 30 minutes late.',\n",
    "        'hypothesis': 'The train was delayed.',\n",
    "        'expected': 'entailment'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'The museum is open on weekends.',\n",
    "        'hypothesis': 'The museum is closed every day.',\n",
    "        'expected': 'contradiction'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'A woman is walking her dog.',\n",
    "        'hypothesis': 'The dog is brown in color.',\n",
    "        'expected': 'neutral'\n",
    "    },\n",
    "    {\n",
    "        'premise': 'The movie theater is showing new releases.',\n",
    "        'hypothesis': 'Films are being screened.',\n",
    "        'expected': 'entailment'\n",
    "    }\n",
    "    ]\n",
    "    \n",
    "    # Test each example\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        result = predictor.predict(example['premise'], example['hypothesis'])\n",
    "        logger.info(f\"\\nExample {i}:\")\n",
    "        logger.info(f\"Premise: {example['premise']}\")\n",
    "        logger.info(f\"Hypothesis: {example['hypothesis']}\")\n",
    "        logger.info(f\"Expected: {example['expected']}\")\n",
    "        logger.info(f\"Predicted: {result['label']}\")\n",
    "        logger.info(\"Probabilities:\")\n",
    "        for label, prob in result['probabilities'].items():\n",
    "            logger.info(f\"  {label}: {prob:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248fe34-662a-4897-85ab-f5df9cee8c7c",
   "metadata": {},
   "source": [
    "# Cool and Interesting Insights!\n",
    "\n",
    "## Confidence Patterns\n",
    "1. High Confidence Successes\n",
    "- Most successful entailment predictions had very high confidence (>0.7)\n",
    "- Example: \"chef preparing pasta\" → \"someone cooking food\" (86.8% confidence)\n",
    "- Example: \"movie theater showing releases\" → \"films being screened\" (71.1% confidence)\n",
    "\n",
    "2. Challenging Cases\n",
    "- Model struggles most with neutral relationships, often misclassifying them as contradictions\n",
    "- Borderline cases show very close probability distributions across all three classes\n",
    "- Example: \"train delayed\" case had almost equal probabilities (35.6%, 35.0%, 29.4%)\n",
    "\n",
    "## Pattern Analysis\n",
    "1. Strong Performance:\n",
    "- Direct logical entailments (cooking→food preparation)\n",
    "- Simple activity relationships (playing soccer→outdoor sports)\n",
    "- Direct contradictions with clear opposing statements\n",
    "\n",
    "2. Common Mistakes:\n",
    "- Over-predicting entailment for temporal relationships\n",
    "- Struggling with neutral cases involving additional details\n",
    "- Difficulty with implicit contradictions\n",
    "\n",
    "## Specific Weaknesses\n",
    "1. Context Understanding:\n",
    "- Failed on \"clear sky\" → \"heavy rain\" (predicted entailment with 77.3% confidence)\n",
    "- Struggled with context-dependent relationships\n",
    "\n",
    "2. Subtle Relationships:\n",
    "- Poor performance on neutral cases requiring world knowledge\n",
    "- Example: \"student writing notes\" → \"understanding material\"\n",
    "- Example: \"laptop purchase\" → \"Amazon purchase\"\n",
    "\n",
    "The model shows promising performance on straightforward relationships but needs improvement in handling subtle distinctions and world knowledge integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70240431-7706-416b-afab-97ae93c3fdea",
   "metadata": {},
   "source": [
    "# Thank You! 🤗"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
