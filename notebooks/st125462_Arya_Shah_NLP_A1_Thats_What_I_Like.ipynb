{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3724e9b2-bca7-4a06-a638-c14ab48881cc",
   "metadata": {},
   "source": [
    "#  AT82.05 Artificial Intelligence: Natural Language Understanding (NLU)\n",
    "\n",
    "## A1: That's What I LIKE!\n",
    "\n",
    "### Name: Arya Shah\n",
    "### StudentID: st125462\n",
    "\n",
    "-----------\n",
    "\n",
    "In this assignment I will focus on creating a system to find similar context in natural language processing. The system, deployed on a website, should return the top paragraphs with the most similar context to a given query, such as ”Harry Potter.” This task will involve building upon existing code, understanding and implementing word embedding techniques, and creating a web interface for the system to deliver the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd9dad-228e-461d-bae5-72bf7dc7fec2",
   "metadata": {},
   "source": [
    "# Task 1: Preparation and Training\n",
    "Build upon the code discussed in class. Do not use pre-built solutions from the internet.\n",
    "\n",
    "1. Read and understand the Word2Vec1 and GloVe2 papers.✅\n",
    "2. Modify the Word2Vec (with & without negative sampling) and GloVe from the lab lecture (3 points)\n",
    "- Train using a real-world corpus (suggest to categories news from nltk datset). Ensure to source this dataset from reputable public databases or repositories. It is imperative to give proper\n",
    "credit to the dataset source in your documentation.✅\n",
    "- Create a function that allows dynamic modification of the window size during training. Use a window size of 2 as default.✅\n",
    "\n",
    "**I make use of the NLTK Brown corpus (as mentioned in the assignment problem statement). Source: https://www.nltk.org/nltk_data/**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced72caa-b6d2-4849-9e39-81ff7499000c",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "The below code helps in facilitating training, logging of results and testing the trainied models by various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f1e1f2-4070-4c40-9e63-08c711fc5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_news_corpus():\n",
    "    \"\"\"Load and preprocess the Brown corpus news category\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('corpora/brown')\n",
    "    except LookupError:\n",
    "        nltk.download('brown')\n",
    "    \n",
    "    # Get news category sentences\n",
    "    news_sents = brown.sents(categories='news')\n",
    "    \n",
    "    # Lowercase and join sentences\n",
    "    corpus = [\" \".join(sent).lower() for sent in news_sents]\n",
    "    return corpus\n",
    "\n",
    "def prepare_vocab(corpus, min_count=5):\n",
    "    \"\"\"Create vocabulary from corpus with minimum frequency threshold\"\"\"\n",
    "    # Tokenize\n",
    "    tokenized = [sent.split() for sent in corpus]\n",
    "    # Count words\n",
    "    word_counts = Counter([word for sent in tokenized for word in sent])\n",
    "    # Filter by minimum count\n",
    "    vocab = [word for word, count in word_counts.items() if count >= min_count]\n",
    "    vocab.append('<UNK>')\n",
    "    \n",
    "    # Create mappings\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    return tokenized, vocab, word2idx, idx2word\n",
    "\n",
    "def load_word_analogies():\n",
    "    \"\"\"Load semantic and syntactic test sets\"\"\"\n",
    "    semantic_file = \"evaluation/capital-common-countries.txt\"\n",
    "    syntactic_file = \"evaluation/past-tense.txt\"\n",
    "    \n",
    "    semantic_pairs = []\n",
    "    syntactic_pairs = []\n",
    "    \n",
    "    # Create evaluation directory if it doesn't exist\n",
    "    os.makedirs(\"evaluation\", exist_ok=True)\n",
    "    \n",
    "    # Create sample semantic analogies (capital-country)\n",
    "    semantic_analogies = [\n",
    "        \"athens greece berlin germany\",\n",
    "        \"athens greece moscow russia\",\n",
    "        \"athens greece paris france\",\n",
    "        \"berlin germany london england\",\n",
    "        \"berlin germany madrid spain\",\n",
    "        \"berlin germany paris france\",\n",
    "        \"london england paris france\",\n",
    "        \"london england rome italy\",\n",
    "        \"madrid spain paris france\",\n",
    "        \"madrid spain rome italy\",\n",
    "        \"paris france rome italy\",\n",
    "        \"rome italy tokyo japan\"\n",
    "    ]\n",
    "    \n",
    "    # Create sample syntactic analogies (verb past tense)\n",
    "    syntactic_analogies = [\n",
    "        \"dance danced smile smiled\",\n",
    "        \"dance danced walk walked\",\n",
    "        \"decrease decreased increase increased\",\n",
    "        \"describe described destroy destroyed\",\n",
    "        \"eat ate speak spoke\",\n",
    "        \"fall fell rise rose\",\n",
    "        \"feed fed speak spoke\",\n",
    "        \"find found lose lost\",\n",
    "        \"go went speak spoke\",\n",
    "        \"grow grew shrink shrank\",\n",
    "        \"lose lost win won\",\n",
    "        \"say said speak spoke\",\n",
    "        \"sing sang write wrote\",\n",
    "        \"sit sat speak spoke\",\n",
    "        \"take took give gave\"\n",
    "    ]\n",
    "    \n",
    "    # Write sample files\n",
    "    with open(semantic_file, 'w') as f:\n",
    "        f.write('\\n'.join(semantic_analogies))\n",
    "    \n",
    "    with open(syntactic_file, 'w') as f:\n",
    "        f.write('\\n'.join(syntactic_analogies))\n",
    "    \n",
    "    # Load and parse files\n",
    "    def load_analogies(filename):\n",
    "        pairs = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                w1, w2, w3, w4 = line.strip().lower().split()\n",
    "                pairs.append((w1, w2, w3, w4))\n",
    "        return pairs\n",
    "    \n",
    "    semantic_pairs = load_analogies(semantic_file)\n",
    "    syntactic_pairs = load_analogies(syntactic_file)\n",
    "    \n",
    "    return semantic_pairs, syntactic_pairs\n",
    "\n",
    "def evaluate_analogies(model, word2idx, idx2word, pairs):\n",
    "    \"\"\"Evaluate word analogy accuracy\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for w1, w2, w3, w4 in pairs:\n",
    "        if w1 not in word2idx or w2 not in word2idx or w3 not in word2idx or w4 not in word2idx:\n",
    "            continue\n",
    "            \n",
    "        # Get embeddings\n",
    "        v1 = model.embedding_center(torch.LongTensor([word2idx[w1]])).detach()\n",
    "        v2 = model.embedding_center(torch.LongTensor([word2idx[w2]])).detach()\n",
    "        v3 = model.embedding_center(torch.LongTensor([word2idx[w3]])).detach()\n",
    "        \n",
    "        # v2 - v1 + v3 should be close to v4\n",
    "        predicted = v2 - v1 + v3\n",
    "        \n",
    "        # Find closest word\n",
    "        distances = []\n",
    "        for idx in range(len(word2idx)):\n",
    "            vec = model.embedding_center(torch.LongTensor([idx])).detach()\n",
    "            dist = torch.nn.functional.cosine_similarity(predicted, vec)\n",
    "            distances.append((dist.item(), idx))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        distances.sort(reverse=True)\n",
    "        \n",
    "        # Get top prediction\n",
    "        pred_word = idx2word[distances[0][1]]\n",
    "        \n",
    "        if pred_word == w4:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "def load_similarity_dataset():\n",
    "    \"\"\"Load the WordSim-353 dataset for word similarity evaluation\"\"\"\n",
    "    wordsim_path = \"evaluation/wordsim353.txt\"\n",
    "    \n",
    "    if not os.path.exists(wordsim_path):\n",
    "        logger.error(f\"WordSim-353 file not found at {wordsim_path}\")\n",
    "        return create_fallback_dataset()\n",
    "    \n",
    "    # Load and parse the dataset\n",
    "    similarities = []\n",
    "    try:\n",
    "        with open(wordsim_path, 'r', encoding='utf-8') as f:\n",
    "            # Read all lines\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            # Check if there's a header and skip if present\n",
    "            start_idx = 0\n",
    "            if lines and any(header in lines[0].lower() for header in ['word1', 'word2', 'score', 'human']):\n",
    "                start_idx = 1\n",
    "            \n",
    "            # Parse each line\n",
    "            for line in lines[start_idx:]:\n",
    "                try:\n",
    "                    # Handle both tab and space-separated formats\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 3:\n",
    "                        word1, word2, score = parts[0], parts[1], float(parts[-1])\n",
    "                        similarities.append((word1.lower(), word2.lower(), float(score)/10))  # Normalize to 0-1\n",
    "                except (ValueError, IndexError) as e:\n",
    "                    logger.warning(f\"Skipping malformed line in similarity dataset: {line.strip()}\")\n",
    "                    continue\n",
    "        \n",
    "        if similarities:\n",
    "            logger.info(f\"Successfully loaded {len(similarities)} word pairs from WordSim-353\")\n",
    "            return similarities\n",
    "        else:\n",
    "            logger.error(\"No valid similarities found in WordSim-353 file\")\n",
    "            return create_fallback_dataset()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading WordSim-353: {e}\")\n",
    "        return create_fallback_dataset()\n",
    "\n",
    "def create_fallback_dataset():\n",
    "    \"\"\"Create a minimal fallback dataset for when WordSim-353 is unavailable\"\"\"\n",
    "    logger.warning(\"Using fallback similarity dataset\")\n",
    "    return [\n",
    "        (\"car\", \"automobile\", 1.0),\n",
    "        (\"gem\", \"jewel\", 0.96),\n",
    "        (\"journey\", \"voyage\", 0.89),\n",
    "        (\"boy\", \"lad\", 0.83),\n",
    "        (\"coast\", \"shore\", 0.79),\n",
    "        (\"asylum\", \"madhouse\", 0.77),\n",
    "        (\"magician\", \"wizard\", 0.73),\n",
    "        (\"midday\", \"noon\", 0.71),\n",
    "        (\"furnace\", \"stove\", 0.69),\n",
    "        (\"food\", \"fruit\", 0.65),\n",
    "    ]\n",
    "\n",
    "def evaluate_similarity(model, word2idx, similarities):\n",
    "    \"\"\"Evaluate model performance on word similarity task\"\"\"\n",
    "    model_sims = []\n",
    "    human_sims = []\n",
    "    num_pairs = 0\n",
    "    \n",
    "    for w1, w2, score in similarities:\n",
    "        if w1 not in word2idx or w2 not in word2idx:\n",
    "            continue\n",
    "            \n",
    "        # Get word vectors\n",
    "        v1 = model.embedding_center(torch.tensor([word2idx[w1]]))\n",
    "        v2 = model.embedding_center(torch.tensor([word2idx[w2]]))\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = F.cosine_similarity(v1, v2).item()\n",
    "        \n",
    "        model_sims.append(cos_sim)\n",
    "        human_sims.append(score)\n",
    "        num_pairs += 1\n",
    "    \n",
    "    if len(model_sims) > 1:\n",
    "        # Calculate correlation and MSE\n",
    "        correlation = spearmanr(model_sims, human_sims)[0]  # Take only the correlation value, not p-value\n",
    "        mse = mean_squared_error(human_sims, model_sims)\n",
    "        return correlation, mse, num_pairs\n",
    "    return 0.0, 0.0, 0\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Class to evaluate and compare different word embedding models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.similarities = load_similarity_dataset()\n",
    "        self.semantic_pairs, self.syntactic_pairs = load_word_analogies()\n",
    "    \n",
    "    def evaluate_model(self, model, word2idx, idx2word, model_name, window_size=None, training_time=None, final_loss=None):\n",
    "        \"\"\"Evaluate a single model and store its results\"\"\"\n",
    "        # Evaluate similarities\n",
    "        correlation, mse, num_pairs = evaluate_similarity(model, word2idx, self.similarities)\n",
    "        \n",
    "        # Evaluate analogies\n",
    "        semantic_acc = evaluate_analogies(model, word2idx, idx2word, self.semantic_pairs)\n",
    "        syntactic_acc = evaluate_analogies(model, word2idx, idx2word, self.syntactic_pairs)\n",
    "        \n",
    "        self.results[model_name] = {\n",
    "            'window_size': window_size,\n",
    "            'training_time': training_time,\n",
    "            'final_loss': final_loss,\n",
    "            'correlation': correlation,\n",
    "            'mse': mse,\n",
    "            'num_pairs': num_pairs,\n",
    "            'semantic_acc': semantic_acc,\n",
    "            'syntactic_acc': syntactic_acc\n",
    "        }\n",
    "    \n",
    "    def print_training_table(self):\n",
    "        \"\"\"Print a table comparing training metrics and accuracy\"\"\"\n",
    "        # Headers\n",
    "        headers = ['Model', 'Window Size', 'Training Loss', 'Training Time', 'Syntactic Acc', 'Semantic Acc']\n",
    "        col_widths = [max(len(str(h)), 15) for h in headers]\n",
    "        \n",
    "        # Update column widths based on data\n",
    "        for model_name, metrics in self.results.items():\n",
    "            col_widths[0] = max(col_widths[0], len(model_name))\n",
    "            values = [\n",
    "                metrics.get('window_size', 'N/A'),\n",
    "                metrics.get('final_loss', 'N/A'),\n",
    "                metrics.get('training_time', 'N/A'),\n",
    "                metrics['syntactic_acc'],\n",
    "                metrics['semantic_acc']\n",
    "            ]\n",
    "            for i, value in enumerate(values):\n",
    "                col_widths[i+1] = max(col_widths[i+1], len(f'{value:.4f}' if isinstance(value, float) else str(value)))\n",
    "        \n",
    "        # Print header\n",
    "        header_line = ' | '.join(h.ljust(w) for h, w in zip(headers, col_widths))\n",
    "        separator = '-' * len(header_line)\n",
    "        print('\\nTraining and Accuracy Results:')\n",
    "        print(separator)\n",
    "        print(header_line)\n",
    "        print(separator)\n",
    "        \n",
    "        # Print each model's results\n",
    "        for model_name, metrics in self.results.items():\n",
    "            row = [\n",
    "                model_name.ljust(col_widths[0]),\n",
    "                str(metrics.get('window_size', 'N/A')).ljust(col_widths[1]),\n",
    "                f\"{metrics.get('final_loss', 'N/A'):.4f}\".ljust(col_widths[2]) if isinstance(metrics.get('final_loss'), float) else 'N/A'.ljust(col_widths[2]),\n",
    "                f\"{metrics.get('training_time', 'N/A'):.2f}s\".ljust(col_widths[3]) if isinstance(metrics.get('training_time'), float) else 'N/A'.ljust(col_widths[3]),\n",
    "                f\"{metrics['syntactic_acc']:.4f}\".ljust(col_widths[4]),\n",
    "                f\"{metrics['semantic_acc']:.4f}\".ljust(col_widths[5])\n",
    "            ]\n",
    "            print(' | '.join(row))\n",
    "        print(separator)\n",
    "    \n",
    "    def print_similarity_table(self):\n",
    "        \"\"\"Print a table comparing similarity metrics against human judgments\"\"\"\n",
    "        # Get unique model types\n",
    "        model_types = {\n",
    "            name.split()[0]: [] for name in self.results.keys()\n",
    "        }\n",
    "        \n",
    "        # Group results by model type\n",
    "        for model_name, metrics in self.results.items():\n",
    "            model_type = model_name.split()[0]\n",
    "            model_types[model_type].append((model_name, metrics))\n",
    "        \n",
    "        # Headers\n",
    "        headers = ['Metric'] + list(model_types.keys()) + ['Y true']\n",
    "        col_widths = [max(len(str(h)), 15) for h in headers]\n",
    "        \n",
    "        # Print header\n",
    "        header_line = ' | '.join(h.ljust(w) for h, w in zip(headers, col_widths))\n",
    "        separator = '-' * len(header_line)\n",
    "        print('\\nSimilarity Comparison Results:')\n",
    "        print(separator)\n",
    "        print(header_line)\n",
    "        print(separator)\n",
    "        \n",
    "        # Print MSE row\n",
    "        mse_row = ['MSE'.ljust(col_widths[0])]\n",
    "        for model_type in model_types:\n",
    "            # Get best MSE for this model type\n",
    "            best_mse = min((m['mse'] for _, m in model_types[model_type]), default='N/A')\n",
    "            mse_row.append(f\"{best_mse:.4f}\".ljust(col_widths[len(mse_row)]) if isinstance(best_mse, float) else 'N/A'.ljust(col_widths[len(mse_row)]))\n",
    "        mse_row.append('1.0000'.ljust(col_widths[-1]))  # Y true column\n",
    "        print(' | '.join(mse_row))\n",
    "        print(separator)\n",
    "    \n",
    "    def get_results_dict(self):\n",
    "        \"\"\"Return the results dictionary for external use\"\"\"\n",
    "        return self.results\n",
    "\n",
    "def save_model(model, word2idx, idx2word, model_path, model_type=None):\n",
    "    \"\"\"Save model and vocabulary mappings\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to save\n",
    "        word2idx: Word to index mapping\n",
    "        idx2word: Index to word mapping\n",
    "        model_path: Base path for saving the model\n",
    "        model_type: Type of model (skipgram, skipgram_neg, glove)\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    \n",
    "    # Add model type to filename if provided\n",
    "    if model_type:\n",
    "        path_parts = os.path.splitext(model_path)\n",
    "        model_path = f\"{path_parts[0]}_{model_type}{path_parts[1]}\"\n",
    "    \n",
    "    # Save the PyTorch model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'word2idx': word2idx,\n",
    "        'idx2word': idx2word,\n",
    "        'embedding_dim': model.embedding_center.embedding_dim,\n",
    "        'vocab_size': len(word2idx),\n",
    "        'model_type': model_type\n",
    "    }, model_path)\n",
    "    logger.info(f\"Model saved to {model_path}\")\n",
    "\n",
    "def load_model(model_class, model_path):\n",
    "    \"\"\"Load model and vocabulary mappings\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    \n",
    "    # Load the saved state\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # Create model instance\n",
    "    model = model_class(checkpoint['vocab_size'], checkpoint['embedding_dim'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    return model, checkpoint['word2idx'], checkpoint['idx2word']\n",
    "\n",
    "def find_similar_words(query, model, word2idx, idx2word, topk=10):\n",
    "    \"\"\"Find top-k similar words for a query using the trained model\"\"\"\n",
    "    if isinstance(query, str):\n",
    "        # Single word query\n",
    "        if query not in word2idx:\n",
    "            return []\n",
    "        query_idx = word2idx[query]\n",
    "        query_vec = model.embedding_center(torch.LongTensor([query_idx])).detach()\n",
    "    else:\n",
    "        # Multiple word query - average the vectors\n",
    "        query_words = query.lower().split()\n",
    "        vectors = []\n",
    "        for word in query_words:\n",
    "            if word in word2idx:\n",
    "                word_idx = word2idx[word]\n",
    "                vectors.append(model.embedding_center(torch.LongTensor([word_idx])).detach())\n",
    "        if not vectors:\n",
    "            return []\n",
    "        query_vec = torch.mean(torch.stack(vectors), dim=0)\n",
    "\n",
    "    # Calculate similarities with all words\n",
    "    similarities = []\n",
    "    for idx in range(len(word2idx)):\n",
    "        vec = model.embedding_center(torch.LongTensor([idx])).detach()\n",
    "        sim = torch.nn.functional.cosine_similarity(query_vec, vec)\n",
    "        similarities.append((idx2word[idx], sim.item()))\n",
    "    \n",
    "    # Sort by similarity and return top k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:topk]\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        self.interval = self.end - self.start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24495b3c-5daa-4198-aef3-035d775d3ea0",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c06c42e-515e-49ad-a724-e0842231cd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:\n",
      "Training Skip-gram with config: {'window_size': 2, 'embedding_size': 100}\n",
      "INFO:__main__:\n",
      "==================== Training Configuration ====================\n",
      "INFO:__main__:Window Size: 2\n",
      "INFO:__main__:Embedding Size: 100\n",
      "INFO:__main__:Batch Size: 128\n",
      "INFO:__main__:Epochs: 5\n",
      "\n",
      "INFO:__main__:Preparing training data...\n",
      "INFO:__main__:Vocabulary size: 2560 words\n",
      "INFO:__main__:Creating skipgrams...\n",
      "Processing sentences: 100%|██████████| 4623/4623 [00:00<00:00, 32061.37it/s]\n",
      "INFO:__main__:Created 374548 skipgrams\n",
      "INFO:__main__:Model parameters: 512,000\n",
      "INFO:__main__:Loading evaluation datasets...\n",
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "INFO:__main__:\n",
      "==================== Starting Training ====================\n",
      "Epoch 1/5: 100%|██████████| 2927/2927 [05:19<00:00,  9.17it/s, loss=11.1077, avg_loss=18.2994]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 1...\n",
      "INFO:__main__:\n",
      "Epoch 1 Summary:\n",
      "INFO:__main__:Average Loss: 18.2994\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0160\n",
      "INFO:__main__:MSE: 0.2838\n",
      "INFO:__main__:Evaluation Time: 0.93s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram.pt\n",
      "Epoch 2/5: 100%|██████████| 2927/2927 [05:13<00:00,  9.34it/s, loss=7.8520, avg_loss=11.2770] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 2...\n",
      "INFO:__main__:\n",
      "Epoch 2 Summary:\n",
      "INFO:__main__:Average Loss: 11.2770\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0385\n",
      "INFO:__main__:MSE: 0.2732\n",
      "INFO:__main__:Evaluation Time: 0.98s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram.pt\n",
      "Epoch 3/5: 100%|██████████| 2927/2927 [05:30<00:00,  8.86it/s, loss=6.4394, avg_loss=8.6451] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 3...\n",
      "INFO:__main__:\n",
      "Epoch 3 Summary:\n",
      "INFO:__main__:Average Loss: 8.6451\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0195\n",
      "INFO:__main__:MSE: 0.2619\n",
      "INFO:__main__:Evaluation Time: 1.11s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram.pt\n",
      "Epoch 4/5: 100%|██████████| 2927/2927 [05:32<00:00,  8.80it/s, loss=5.5201, avg_loss=7.2730] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 4...\n",
      "INFO:__main__:\n",
      "Epoch 4 Summary:\n",
      "INFO:__main__:Average Loss: 7.2730\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0529\n",
      "INFO:__main__:MSE: 0.2542\n",
      "INFO:__main__:Evaluation Time: 1.01s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram.pt\n",
      "Epoch 5/5: 100%|██████████| 2927/2927 [05:32<00:00,  8.79it/s, loss=4.8776, avg_loss=6.4928] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 5...\n",
      "INFO:__main__:\n",
      "Epoch 5 Summary:\n",
      "INFO:__main__:Average Loss: 6.4928\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0710\n",
      "INFO:__main__:MSE: 0.2487\n",
      "INFO:__main__:Evaluation Time: 1.01s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram.pt\n",
      "INFO:__main__:\n",
      "==================== Training Complete ====================\n",
      "INFO:__main__:Total training time: 1633.94s\n",
      "INFO:__main__:Best loss achieved: 6.4928\n",
      "INFO:__main__:\n",
      "Training Skip-gram with config: {'window_size': 5, 'embedding_size': 100}\n",
      "INFO:__main__:\n",
      "==================== Training Configuration ====================\n",
      "INFO:__main__:Window Size: 5\n",
      "INFO:__main__:Embedding Size: 100\n",
      "INFO:__main__:Batch Size: 128\n",
      "INFO:__main__:Epochs: 5\n",
      "\n",
      "INFO:__main__:Preparing training data...\n",
      "INFO:__main__:Vocabulary size: 2560 words\n",
      "INFO:__main__:Creating skipgrams...\n",
      "Processing sentences: 100%|██████████| 4623/4623 [00:00<00:00, 19776.78it/s]\n",
      "INFO:__main__:Created 869448 skipgrams\n",
      "INFO:__main__:Model parameters: 512,000\n",
      "INFO:__main__:Loading evaluation datasets...\n",
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "INFO:__main__:\n",
      "==================== Starting Training ====================\n",
      "Epoch 1/5: 100%|██████████| 6793/6793 [12:45<00:00,  8.88it/s, loss=10.6393, avg_loss=15.1985]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 1...\n",
      "INFO:__main__:\n",
      "Epoch 1 Summary:\n",
      "INFO:__main__:Average Loss: 15.1985\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0425\n",
      "INFO:__main__:MSE: 0.2617\n",
      "INFO:__main__:Evaluation Time: 0.90s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram.pt\n",
      "Epoch 2/5: 100%|██████████| 6793/6793 [12:22<00:00,  9.15it/s, loss=7.0392, avg_loss=8.5793]  \n",
      "INFO:__main__:\n",
      "Evaluating epoch 2...\n",
      "INFO:__main__:\n",
      "Epoch 2 Summary:\n",
      "INFO:__main__:Average Loss: 8.5793\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0057\n",
      "INFO:__main__:MSE: 0.2487\n",
      "INFO:__main__:Evaluation Time: 1.23s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram.pt\n",
      "Epoch 3/5: 100%|██████████| 6793/6793 [13:09<00:00,  8.60it/s, loss=5.9572, avg_loss=6.7026] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 3...\n",
      "INFO:__main__:\n",
      "Epoch 3 Summary:\n",
      "INFO:__main__:Average Loss: 6.7026\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0127\n",
      "INFO:__main__:MSE: 0.2442\n",
      "INFO:__main__:Evaluation Time: 0.89s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram.pt\n",
      "Epoch 4/5: 100%|██████████| 6793/6793 [12:41<00:00,  8.92it/s, loss=5.4494, avg_loss=5.9656]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 4...\n",
      "INFO:__main__:\n",
      "Epoch 4 Summary:\n",
      "INFO:__main__:Average Loss: 5.9656\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0619\n",
      "INFO:__main__:MSE: 0.2451\n",
      "INFO:__main__:Evaluation Time: 0.89s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram.pt\n",
      "Epoch 5/5: 100%|██████████| 6793/6793 [12:30<00:00,  9.05it/s, loss=5.1846, avg_loss=5.6520]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 5...\n",
      "INFO:__main__:\n",
      "Epoch 5 Summary:\n",
      "INFO:__main__:Average Loss: 5.6520\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0731\n",
      "INFO:__main__:MSE: 0.2474\n",
      "INFO:__main__:Evaluation Time: 0.89s\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram.pt\n",
      "INFO:__main__:\n",
      "==================== Training Complete ====================\n",
      "INFO:__main__:Total training time: 3814.83s\n",
      "INFO:__main__:Best loss achieved: 5.6520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Accuracy Results:\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Model           | Window Size     | Training Loss   | Training Time   | Syntactic Acc   | Semantic Acc   \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Skipgram (w=2)  | 2               | 6.4928          | 1633.94s        | 0.0000          | 0.0000         \n",
      "Skipgram (w=5)  | 5               | 5.6520          | 3814.83s        | 0.0000          | 0.0000         \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Similarity Comparison Results:\n",
      "---------------------------------------------------\n",
      "Metric          | Skipgram        | Y true         \n",
      "---------------------------------------------------\n",
      "MSE             | 0.2474          | 1.0000         \n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding = self.embedding_center(center)\n",
    "        outside_embedding = self.embedding_outside(outside)\n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))\n",
    "        return loss\n",
    "\n",
    "def create_skipgrams(sentence, window_size):\n",
    "    skipgrams = []\n",
    "    for i in range(len(sentence)):\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_pos = i + w\n",
    "            if context_pos < 0 or context_pos >= len(sentence) or context_pos == i:\n",
    "                continue\n",
    "            skipgrams.append((sentence[i], sentence[context_pos]))\n",
    "    return skipgrams\n",
    "\n",
    "def prepare_batch(skipgrams, batch_size, word2idx, vocab_size):\n",
    "    # Random sample from skipgrams\n",
    "    indices = np.random.choice(len(skipgrams), batch_size, replace=False)\n",
    "    \n",
    "    centers = [[word2idx.get(skipgrams[i][0], word2idx['<UNK>'])] for i in indices]\n",
    "    outsides = [[word2idx.get(skipgrams[i][1], word2idx['<UNK>'])] for i in indices]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    centers = torch.LongTensor(centers)\n",
    "    outsides = torch.LongTensor(outsides)\n",
    "    all_vocabs = torch.arange(vocab_size).expand(batch_size, vocab_size)\n",
    "    \n",
    "    return centers, outsides, all_vocabs\n",
    "\n",
    "def train(corpus, window_size=2, embedding_size=100, batch_size=128, epochs=5):\n",
    "    logger.info(f\"\\n{'='*20} Training Configuration {'='*20}\")\n",
    "    logger.info(f\"Window Size: {window_size}\")\n",
    "    logger.info(f\"Embedding Size: {embedding_size}\")\n",
    "    logger.info(f\"Batch Size: {batch_size}\")\n",
    "    logger.info(f\"Epochs: {epochs}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    logger.info(\"Preparing training data...\")\n",
    "    tokenized, vocab, word2idx, idx2word = prepare_vocab(corpus)\n",
    "    logger.info(f\"Vocabulary size: {len(vocab)} words\")\n",
    "    \n",
    "    # Create skipgrams\n",
    "    logger.info(\"Creating skipgrams...\")\n",
    "    all_skipgrams = []\n",
    "    for sentence in tqdm(tokenized, desc=\"Processing sentences\"):\n",
    "        all_skipgrams.extend(create_skipgrams(sentence, window_size))\n",
    "    logger.info(f\"Created {len(all_skipgrams)} skipgrams\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Skipgram(len(vocab), embedding_size)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Load evaluation datasets\n",
    "    logger.info(\"Loading evaluation datasets...\")\n",
    "    semantic_pairs, syntactic_pairs = load_word_analogies()\n",
    "    similarities = load_similarity_dataset()\n",
    "    logger.info(f\"Loaded {len(semantic_pairs)} semantic pairs and {len(syntactic_pairs)} syntactic pairs\")\n",
    "    \n",
    "    # Training metrics\n",
    "    best_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logger.info(f\"\\n{'='*20} Starting Training {'='*20}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        num_batches = len(all_skipgrams) // batch_size + (1 if len(all_skipgrams) % batch_size != 0 else 0)\n",
    "        pbar = tqdm(range(0, len(all_skipgrams), batch_size), \n",
    "                   desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "                   total=num_batches)\n",
    "        \n",
    "        for i in pbar:\n",
    "            # Prepare batch\n",
    "            centers, outsides, all_vocabs = prepare_batch(\n",
    "                all_skipgrams[i:i+batch_size],\n",
    "                min(batch_size, len(all_skipgrams) - i),\n",
    "                word2idx,\n",
    "                len(vocab)\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = model(centers, outsides, all_vocabs)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            current_loss = loss.item()\n",
    "            epoch_loss += current_loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{current_loss:.4f}',\n",
    "                'avg_loss': f'{epoch_loss/batch_count:.4f}'\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        \n",
    "        # Evaluate model\n",
    "        logger.info(f\"\\nEvaluating epoch {epoch+1}...\")\n",
    "        with Timer() as eval_timer:\n",
    "            semantic_acc = evaluate_analogies(model, word2idx, idx2word, semantic_pairs)\n",
    "            syntactic_acc = evaluate_analogies(model, word2idx, idx2word, syntactic_pairs)\n",
    "            similarity_corr, mse, num_pairs = evaluate_similarity(model, word2idx, similarities)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        logger.info(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        logger.info(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        logger.info(f\"Semantic Accuracy: {semantic_acc:.4f}\")\n",
    "        logger.info(f\"Syntactic Accuracy: {syntactic_acc:.4f}\")\n",
    "        logger.info(f\"Similarity Correlation: {similarity_corr:.4f}\")\n",
    "        logger.info(f\"MSE: {mse:.4f}\")\n",
    "        logger.info(f\"Evaluation Time: {eval_timer.interval:.2f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            logger.info(\"New best model! Saving checkpoint...\")\n",
    "            model_dir = \"saved_models\"\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model_path = os.path.join(model_dir, f\"w{window_size}_e{embedding_size}.pt\")\n",
    "            save_model(model, word2idx, idx2word, model_path, model_type=\"skipgram\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    logger.info(f\"\\n{'='*20} Training Complete {'='*20}\")\n",
    "    logger.info(f\"Total training time: {training_time:.2f}s\")\n",
    "    logger.info(f\"Best loss achieved: {best_loss:.4f}\")\n",
    "    \n",
    "    return model, {\n",
    "        'final_loss': avg_loss,\n",
    "        'best_loss': best_loss,\n",
    "        'training_time': training_time,\n",
    "        'semantic_accuracy': semantic_acc,\n",
    "        'syntactic_accuracy': syntactic_acc,\n",
    "        'similarity_correlation': similarity_corr,\n",
    "        'mse': mse,\n",
    "        'num_pairs': num_pairs,\n",
    "        'model_path': model_path,\n",
    "        'word2idx': word2idx,\n",
    "        'idx2word': idx2word\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load corpus\n",
    "    corpus = load_news_corpus()\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    # Train models with different configurations\n",
    "    configs = [\n",
    "        {'window_size': 2, 'embedding_size': 100},\n",
    "        {'window_size': 5, 'embedding_size': 100}\n",
    "    ]\n",
    "    \n",
    "    for config in configs:\n",
    "        logger.info(f\"\\nTraining Skip-gram with config: {config}\")\n",
    "        model, results = train(corpus, **config)\n",
    "        \n",
    "        model_name = f\"Skipgram (w={config['window_size']})\"\n",
    "        evaluator.evaluate_model(\n",
    "            model, \n",
    "            results['word2idx'], \n",
    "            results['idx2word'], \n",
    "            model_name,\n",
    "            window_size=config['window_size'],\n",
    "            training_time=results['training_time'],\n",
    "            final_loss=results['final_loss']\n",
    "        )\n",
    "    \n",
    "    # Print both tables\n",
    "    evaluator.print_training_table()\n",
    "    evaluator.print_similarity_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa366cc0-6375-4938-9023-9776c9223826",
   "metadata": {},
   "source": [
    "Trying the similar words test function for skip-gram model. Later part of the code has the testing done for all models in one script itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dc803ae-869d-4166-a85e-434e4e6f3fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model from: /home/jupyter-st125462/NLP/A1/saved_models/w2_e100_skipgram.pt\n",
      "/tmp/ipykernel_762838/3318003762.py:379: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n",
      "WARNING:__main__:Word 'king' not in vocabulary\n",
      "WARNING:__main__:Word 'computer' not in vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar words to 'king':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Similar words to 'computer':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Similar words to 'good':\n",
      "------------------------------------------------------------\n",
      "+--------+------------+--------------+\n",
      "|   Rank | Word       |   Similarity |\n",
      "+========+============+==============+\n",
      "|      2 | brevard    |       0.3579 |\n",
      "+--------+------------+--------------+\n",
      "|      3 | nomination |       0.3514 |\n",
      "+--------+------------+--------------+\n",
      "|      4 | setting    |       0.3222 |\n",
      "+--------+------------+--------------+\n",
      "|      5 | important  |       0.3195 |\n",
      "+--------+------------+--------------+\n",
      "|      6 | c.         |       0.3181 |\n",
      "+--------+------------+--------------+\n",
      "|      7 | group      |       0.3123 |\n",
      "+--------+------------+--------------+\n",
      "|      8 | increased  |       0.3071 |\n",
      "+--------+------------+--------------+\n",
      "|      9 | table      |       0.3063 |\n",
      "+--------+------------+--------------+\n",
      "|     10 | clark      |       0.3012 |\n",
      "+--------+------------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'day':\n",
      "------------------------------------------------------------\n",
      "+--------+-------------+--------------+\n",
      "|   Rank | Word        |   Similarity |\n",
      "+========+=============+==============+\n",
      "|      2 | christmas   |       0.3743 |\n",
      "+--------+-------------+--------------+\n",
      "|      3 | problems    |       0.3686 |\n",
      "+--------+-------------+--------------+\n",
      "|      4 | calls       |       0.348  |\n",
      "+--------+-------------+--------------+\n",
      "|      5 | address     |       0.3348 |\n",
      "+--------+-------------+--------------+\n",
      "|      6 | order       |       0.3309 |\n",
      "+--------+-------------+--------------+\n",
      "|      7 | immediate   |       0.3283 |\n",
      "+--------+-------------+--------------+\n",
      "|      8 | p.m.        |       0.3258 |\n",
      "+--------+-------------+--------------+\n",
      "|      9 | opportunity |       0.3223 |\n",
      "+--------+-------------+--------------+\n",
      "|     10 | informed    |       0.3212 |\n",
      "+--------+-------------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'time':\n",
      "------------------------------------------------------------\n",
      "+--------+----------+--------------+\n",
      "|   Rank | Word     |   Similarity |\n",
      "+========+==========+==============+\n",
      "|      2 | not      |       0.5119 |\n",
      "+--------+----------+--------------+\n",
      "|      3 | victory  |       0.398  |\n",
      "+--------+----------+--------------+\n",
      "|      4 | will     |       0.3484 |\n",
      "+--------+----------+--------------+\n",
      "|      5 | about    |       0.3436 |\n",
      "+--------+----------+--------------+\n",
      "|      6 | large    |       0.3395 |\n",
      "+--------+----------+--------------+\n",
      "|      7 | homes    |       0.3289 |\n",
      "+--------+----------+--------------+\n",
      "|      8 | raising  |       0.3279 |\n",
      "+--------+----------+--------------+\n",
      "|      9 | proposed |       0.3273 |\n",
      "+--------+----------+--------------+\n",
      "|     10 | previous |       0.3266 |\n",
      "+--------+----------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'person':\n",
      "------------------------------------------------------------\n",
      "+--------+-----------+--------------+\n",
      "|   Rank | Word      |   Similarity |\n",
      "+========+===========+==============+\n",
      "|      2 | 1959      |       0.4341 |\n",
      "+--------+-----------+--------------+\n",
      "|      3 | comes     |       0.3612 |\n",
      "+--------+-----------+--------------+\n",
      "|      4 | nine      |       0.3565 |\n",
      "+--------+-----------+--------------+\n",
      "|      5 | produced  |       0.3518 |\n",
      "+--------+-----------+--------------+\n",
      "|      6 | cost      |       0.3374 |\n",
      "+--------+-----------+--------------+\n",
      "|      7 | personnel |       0.3316 |\n",
      "+--------+-----------+--------------+\n",
      "|      8 | over      |       0.3164 |\n",
      "+--------+-----------+--------------+\n",
      "|      9 | board     |       0.3083 |\n",
      "+--------+-----------+--------------+\n",
      "|     10 | only      |       0.308  |\n",
      "+--------+-----------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'world':\n",
      "------------------------------------------------------------\n",
      "+--------+----------+--------------+\n",
      "|   Rank | Word     |   Similarity |\n",
      "+========+==========+==============+\n",
      "|      2 | opinion  |       0.4198 |\n",
      "+--------+----------+--------------+\n",
      "|      3 | marr     |       0.372  |\n",
      "+--------+----------+--------------+\n",
      "|      4 | he       |       0.3483 |\n",
      "+--------+----------+--------------+\n",
      "|      5 | long     |       0.3454 |\n",
      "+--------+----------+--------------+\n",
      "|      6 | entering |       0.3378 |\n",
      "+--------+----------+--------------+\n",
      "|      7 | scene    |       0.3341 |\n",
      "+--------+----------+--------------+\n",
      "|      8 | last     |       0.3122 |\n",
      "+--------+----------+--------------+\n",
      "|      9 | very     |       0.3119 |\n",
      "+--------+----------+--------------+\n",
      "|     10 | i        |       0.3063 |\n",
      "+--------+----------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'work':\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Word 'data' not in vocabulary\n",
      "WARNING:__main__:Word 'algorithm' not in vocabulary\n",
      "WARNING:__main__:Word 'network' not in vocabulary\n",
      "WARNING:__main__:Word 'python' not in vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+\n",
      "|   Rank | Word     |   Similarity |\n",
      "+========+==========+==============+\n",
      "|      2 | income   |       0.3792 |\n",
      "+--------+----------+--------------+\n",
      "|      3 | for      |       0.3658 |\n",
      "+--------+----------+--------------+\n",
      "|      4 | earnings |       0.356  |\n",
      "+--------+----------+--------------+\n",
      "|      5 | posts    |       0.3471 |\n",
      "+--------+----------+--------------+\n",
      "|      6 | bob      |       0.3403 |\n",
      "+--------+----------+--------------+\n",
      "|      7 | post     |       0.3355 |\n",
      "+--------+----------+--------------+\n",
      "|      8 | camp     |       0.3307 |\n",
      "+--------+----------+--------------+\n",
      "|      9 | orders   |       0.3253 |\n",
      "+--------+----------+--------------+\n",
      "|     10 | out      |       0.3233 |\n",
      "+--------+----------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'data':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Similar words to 'algorithm':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Similar words to 'network':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Similar words to 'science':\n",
      "------------------------------------------------------------\n",
      "+--------+-----------+--------------+\n",
      "|   Rank | Word      |   Similarity |\n",
      "+========+===========+==============+\n",
      "|      2 | works     |       0.3314 |\n",
      "+--------+-----------+--------------+\n",
      "|      3 | congolese |       0.32   |\n",
      "+--------+-----------+--------------+\n",
      "|      4 | sheriff   |       0.3009 |\n",
      "+--------+-----------+--------------+\n",
      "|      5 | weather   |       0.2996 |\n",
      "+--------+-----------+--------------+\n",
      "|      6 | stressed  |       0.289  |\n",
      "+--------+-----------+--------------+\n",
      "|      7 | right     |       0.2847 |\n",
      "+--------+-----------+--------------+\n",
      "|      8 | driven    |       0.2812 |\n",
      "+--------+-----------+--------------+\n",
      "|      9 | declared  |       0.2792 |\n",
      "+--------+-----------+--------------+\n",
      "|     10 | leader    |       0.278  |\n",
      "+--------+-----------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'python':\n",
      "------------------------------------------------------------\n",
      "\n",
      "Similar words to 'machine':\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Word 'artificial' not in vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------+\n",
      "|   Rank | Word        |   Similarity |\n",
      "+========+=============+==============+\n",
      "|      2 | hope        |       0.3684 |\n",
      "+--------+-------------+--------------+\n",
      "|      3 | threat      |       0.3361 |\n",
      "+--------+-------------+--------------+\n",
      "|      4 | people      |       0.3242 |\n",
      "+--------+-------------+--------------+\n",
      "|      5 | boston      |       0.3204 |\n",
      "+--------+-------------+--------------+\n",
      "|      6 | recommended |       0.3133 |\n",
      "+--------+-------------+--------------+\n",
      "|      7 | yesterday   |       0.3086 |\n",
      "+--------+-------------+--------------+\n",
      "|      8 | under       |       0.3083 |\n",
      "+--------+-------------+--------------+\n",
      "|      9 | kitchen     |       0.2988 |\n",
      "+--------+-------------+--------------+\n",
      "|     10 | business    |       0.2967 |\n",
      "+--------+-------------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'learning':\n",
      "------------------------------------------------------------\n",
      "+--------+-----------+--------------+\n",
      "|   Rank | Word      |   Similarity |\n",
      "+========+===========+==============+\n",
      "|      2 | rose      |       0.3376 |\n",
      "+--------+-----------+--------------+\n",
      "|      3 | criminal  |       0.3365 |\n",
      "+--------+-----------+--------------+\n",
      "|      4 | wisdom    |       0.3349 |\n",
      "+--------+-----------+--------------+\n",
      "|      5 | tshombe   |       0.3134 |\n",
      "+--------+-----------+--------------+\n",
      "|      6 | time      |       0.3044 |\n",
      "+--------+-----------+--------------+\n",
      "|      7 | going     |       0.3033 |\n",
      "+--------+-----------+--------------+\n",
      "|      8 | each      |       0.2848 |\n",
      "+--------+-----------+--------------+\n",
      "|      9 | joan      |       0.2836 |\n",
      "+--------+-----------+--------------+\n",
      "|     10 | furniture |       0.2828 |\n",
      "+--------+-----------+--------------+\n",
      "\n",
      "\n",
      "Similar words to 'artificial':\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def display_similar_words(query_words, model, word2idx, idx2word, top_k=10):\n",
    "    \"\"\"Display similar words for the Skip-gram model\n",
    "    \n",
    "    Args:\n",
    "        query_words (list): List of words to find similar words for\n",
    "        model: The Skip-gram model\n",
    "        word2idx (dict): Word to index mapping\n",
    "        idx2word (dict): Index to word mapping\n",
    "        top_k (int): Number of similar words to display\n",
    "    \"\"\"\n",
    "    for query in query_words:\n",
    "        print(f\"\\nSimilar words to '{query}':\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if query not in word2idx:\n",
    "            logger.warning(f\"Word '{query}' not in vocabulary\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Use the find_similar_words function from utils.py\n",
    "            similar_words = find_similar_words(query, model, word2idx, idx2word, top_k)\n",
    "            \n",
    "            if not similar_words:\n",
    "                logger.warning(f\"No similar words found for '{query}'\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare table data\n",
    "            table_data = []\n",
    "            for i, (word, sim) in enumerate(similar_words, 1):\n",
    "                if word != query:  # Don't show the query word itself\n",
    "                    table_data.append([f\"{i}\", word, f\"{sim:.4f}\"])\n",
    "            \n",
    "            # Print table\n",
    "            headers = [\"Rank\", \"Word\", \"Similarity\"]\n",
    "            print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error finding similar words: {str(e)}\")\n",
    "            continue\n",
    "        print()\n",
    "\n",
    "def main():\n",
    "    # Model path - update this to your actual model path\n",
    "    model_path = \"/home/jupyter-st125462/NLP/A1/saved_models/w2_e100_skipgram.pt\"\n",
    "    \n",
    "    # Load Skip-gram model\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_path}\")\n",
    "        model, word2idx, idx2word = load_model(Skipgram, model_path)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Query words to test\n",
    "    query_words = [\n",
    "        # Common words\n",
    "        \"king\", \"computer\", \"good\", \"day\", \"time\", \"person\", \"world\", \"work\",\n",
    "        # Domain-specific\n",
    "        \"data\", \"algorithm\", \"network\", \"science\",\n",
    "        # Technical terms\n",
    "        \"python\", \"machine\", \"learning\", \"artificial\"\n",
    "    ]\n",
    "    \n",
    "    # Display similar words\n",
    "    display_similar_words(query_words, model, word2idx, idx2word)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304283ca-ece5-458f-8776-f8818e1c7f57",
   "metadata": {},
   "source": [
    "## Skip-gram (Negative Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "882acda9-c40a-4c36-9798-0a66a76497e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:\n",
      "Training Skip-gram Negative Sampling with config: {'window_size': 2, 'embedding_size': 100, 'neg_samples': 5, 'batch_size': 128, 'epochs': 5}\n",
      "INFO:__main__:\n",
      "==================== Training Configuration ====================\n",
      "INFO:__main__:Window Size: 2\n",
      "INFO:__main__:Embedding Size: 100\n",
      "INFO:__main__:Negative Samples: 5\n",
      "INFO:__main__:Batch Size: 128\n",
      "INFO:__main__:Epochs: 5\n",
      "\n",
      "INFO:__main__:Preparing training data...\n",
      "INFO:__main__:Vocabulary size: 2560 words\n",
      "INFO:__main__:Creating skipgrams...\n",
      "Processing sentences: 100%|██████████| 4623/4623 [00:00<00:00, 31570.27it/s]\n",
      "INFO:__main__:Created 374548 skipgrams\n",
      "INFO:__main__:Creating unigram table...\n",
      "INFO:__main__:Created unigram table with 538072 entries\n",
      "INFO:__main__:Loading evaluation datasets...\n",
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "INFO:__main__:Model parameters: 512,000\n",
      "INFO:__main__:\n",
      "==================== Starting Training ====================\n",
      "Epoch 1/5: 100%|██████████| 2927/2927 [00:29<00:00, 97.68it/s, loss=10.9655] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 1...\n",
      "INFO:__main__:\n",
      "Epoch 1 Summary:\n",
      "INFO:__main__:Average Loss: 13.9636\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.1466\n",
      "INFO:__main__:MSE: 0.2538\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram_neg_skipgram_neg.pt\n",
      "Epoch 2/5: 100%|██████████| 2927/2927 [00:30<00:00, 96.98it/s, loss=9.7618]  \n",
      "INFO:__main__:\n",
      "Evaluating epoch 2...\n",
      "INFO:__main__:\n",
      "Epoch 2 Summary:\n",
      "INFO:__main__:Average Loss: 7.4460\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.1628\n",
      "INFO:__main__:MSE: 0.2419\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram_neg_skipgram_neg.pt\n",
      "Epoch 3/5: 100%|██████████| 2927/2927 [00:30<00:00, 95.96it/s, loss=5.5101] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 3...\n",
      "INFO:__main__:\n",
      "Epoch 3 Summary:\n",
      "INFO:__main__:Average Loss: 4.7757\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.1648\n",
      "INFO:__main__:MSE: 0.2222\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram_neg_skipgram_neg.pt\n",
      "Epoch 4/5: 100%|██████████| 2927/2927 [00:30<00:00, 96.97it/s, loss=3.8024] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 4...\n",
      "INFO:__main__:\n",
      "Epoch 4 Summary:\n",
      "INFO:__main__:Average Loss: 3.4547\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.2017\n",
      "INFO:__main__:MSE: 0.2021\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram_neg_skipgram_neg.pt\n",
      "Epoch 5/5: 100%|██████████| 2927/2927 [00:29<00:00, 97.97it/s, loss=2.5063] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 5...\n",
      "INFO:__main__:\n",
      "Epoch 5 Summary:\n",
      "INFO:__main__:Average Loss: 2.7663\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.1797\n",
      "INFO:__main__:MSE: 0.1879\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_skipgram_neg_skipgram_neg.pt\n",
      "INFO:__main__:Training Time: 155.74s\n",
      "INFO:__main__:\n",
      "Training Skip-gram Negative Sampling with config: {'window_size': 5, 'embedding_size': 100, 'neg_samples': 10, 'batch_size': 128, 'epochs': 5}\n",
      "INFO:__main__:\n",
      "==================== Training Configuration ====================\n",
      "INFO:__main__:Window Size: 5\n",
      "INFO:__main__:Embedding Size: 100\n",
      "INFO:__main__:Negative Samples: 10\n",
      "INFO:__main__:Batch Size: 128\n",
      "INFO:__main__:Epochs: 5\n",
      "\n",
      "INFO:__main__:Preparing training data...\n",
      "INFO:__main__:Vocabulary size: 2560 words\n",
      "INFO:__main__:Creating skipgrams...\n",
      "Processing sentences: 100%|██████████| 4623/4623 [00:00<00:00, 22106.04it/s]\n",
      "INFO:__main__:Created 869448 skipgrams\n",
      "INFO:__main__:Creating unigram table...\n",
      "INFO:__main__:Created unigram table with 538072 entries\n",
      "INFO:__main__:Loading evaluation datasets...\n",
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "INFO:__main__:Model parameters: 512,000\n",
      "INFO:__main__:\n",
      "==================== Starting Training ====================\n",
      "Epoch 1/5: 100%|██████████| 6793/6793 [01:14<00:00, 91.59it/s, loss=10.5298] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 1...\n",
      "INFO:__main__:\n",
      "Epoch 1 Summary:\n",
      "INFO:__main__:Average Loss: 16.6846\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.1166\n",
      "INFO:__main__:MSE: 0.2373\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram_neg_skipgram_neg.pt\n",
      "Epoch 2/5: 100%|██████████| 6793/6793 [01:14<00:00, 90.71it/s, loss=3.4957]  \n",
      "INFO:__main__:\n",
      "Evaluating epoch 2...\n",
      "INFO:__main__:\n",
      "Epoch 2 Summary:\n",
      "INFO:__main__:Average Loss: 5.2603\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.1013\n",
      "INFO:__main__:MSE: 0.1965\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram_neg_skipgram_neg.pt\n",
      "Epoch 3/5: 100%|██████████| 6793/6793 [01:13<00:00, 92.56it/s, loss=3.2153] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 3...\n",
      "INFO:__main__:\n",
      "Epoch 3 Summary:\n",
      "INFO:__main__:Average Loss: 3.2990\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.0730\n",
      "INFO:__main__:MSE: 0.1860\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram_neg_skipgram_neg.pt\n",
      "Epoch 4/5: 100%|██████████| 6793/6793 [01:14<00:00, 90.89it/s, loss=2.2040] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 4...\n",
      "INFO:__main__:\n",
      "Epoch 4 Summary:\n",
      "INFO:__main__:Average Loss: 2.7308\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.1128\n",
      "INFO:__main__:MSE: 0.1935\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram_neg_skipgram_neg.pt\n",
      "Epoch 5/5: 100%|██████████| 6793/6793 [01:12<00:00, 93.74it/s, loss=1.9465] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 5...\n",
      "INFO:__main__:\n",
      "Epoch 5 Summary:\n",
      "INFO:__main__:Average Loss: 2.4614\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.1201\n",
      "INFO:__main__:MSE: 0.2002\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_skipgram_neg_skipgram_neg.pt\n",
      "INFO:__main__:Training Time: 374.31s\n",
      "INFO:__main__:\n",
      "Training Metrics:\n",
      "INFO:__main__:\n",
      "Similarity Metrics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Accuracy Results:\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Model                    | Window Size     | Training Loss   | Training Time   | Syntactic Acc   | Semantic Acc   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Skipgram-NEG (w=2, n=5)  | 2               | 2.7663          | 155.74s         | 0.0000          | 0.0000         \n",
      "Skipgram-NEG (w=5, n=10) | 5               | 2.4614          | 374.31s         | 0.0000          | 0.0000         \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Similarity Comparison Results:\n",
      "---------------------------------------------------\n",
      "Metric          | Skipgram-NEG    | Y true         \n",
      "---------------------------------------------------\n",
      "MSE             | 0.1879          | 1.0000         \n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SkipgramNeg(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        # Get embeddings\n",
    "        center_embed = self.embedding_center(center)\n",
    "        outside_embed = self.embedding_outside(outside)\n",
    "        neg_embed = self.embedding_outside(negative)\n",
    "        \n",
    "        # Positive score\n",
    "        pos_score = self.logsigmoid(torch.sum(outside_embed * center_embed, dim=2)).squeeze()\n",
    "        \n",
    "        # Negative score\n",
    "        neg_score = self.logsigmoid(-torch.bmm(neg_embed, center_embed.transpose(1, 2)).squeeze())\n",
    "        neg_score = torch.sum(neg_score, dim=1)\n",
    "        \n",
    "        loss = -(pos_score + neg_score).mean()\n",
    "        return loss\n",
    "\n",
    "def create_unigram_table(word_counts, vocab_size, table_size=1e6):\n",
    "    pow_freq = np.array(list(word_counts.values())) ** 0.75\n",
    "    power_sum = sum(pow_freq)\n",
    "    ratio = pow_freq / power_sum\n",
    "    count = np.round(ratio * table_size)\n",
    "    \n",
    "    table = []\n",
    "    for idx, x in enumerate(count):\n",
    "        # Ensure idx is within vocabulary range\n",
    "        if idx < vocab_size:\n",
    "            table.extend([idx] * int(x))\n",
    "    return table\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k, vocab_size):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        negs = []\n",
    "        target_idx = targets[i].item()\n",
    "        while len(negs) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            # Make sure the negative sample is within vocabulary range\n",
    "            if neg != target_idx and neg < vocab_size:\n",
    "                negs.append(neg)\n",
    "        neg_samples.append(negs)\n",
    "    \n",
    "    return torch.LongTensor(neg_samples)\n",
    "\n",
    "def create_skipgrams(sentence, window_size):\n",
    "    skipgrams = []\n",
    "    for i in range(len(sentence)):\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_pos = i + w\n",
    "            if context_pos < 0 or context_pos >= len(sentence) or context_pos == i:\n",
    "                continue\n",
    "            skipgrams.append((sentence[i], sentence[context_pos]))\n",
    "    return skipgrams\n",
    "\n",
    "def prepare_batch(skipgrams, batch_size, word2idx, unigram_table, neg_samples=5):\n",
    "    # Random sample from skipgrams\n",
    "    indices = np.random.choice(len(skipgrams), batch_size, replace=False)\n",
    "    \n",
    "    centers = [[word2idx.get(skipgrams[i][0], word2idx['<UNK>'])] for i in indices]\n",
    "    outsides = [[word2idx.get(skipgrams[i][1], word2idx['<UNK>'])] for i in indices]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    centers = torch.LongTensor(centers)\n",
    "    outsides = torch.LongTensor(outsides)\n",
    "    \n",
    "    # Generate negative samples\n",
    "    negative = negative_sampling(outsides.squeeze(), unigram_table, neg_samples, len(word2idx))\n",
    "    \n",
    "    return centers, outsides, negative\n",
    "\n",
    "def train(corpus, window_size=2, embedding_size=100, neg_samples=5, batch_size=128, epochs=5):\n",
    "    \"\"\"Train the Skip-gram model with negative sampling\"\"\"\n",
    "    logger.info(f\"\\n{'='*20} Training Configuration {'='*20}\")\n",
    "    logger.info(f\"Window Size: {window_size}\")\n",
    "    logger.info(f\"Embedding Size: {embedding_size}\")\n",
    "    logger.info(f\"Negative Samples: {neg_samples}\")\n",
    "    logger.info(f\"Batch Size: {batch_size}\")\n",
    "    logger.info(f\"Epochs: {epochs}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    logger.info(\"Preparing training data...\")\n",
    "    tokenized, vocab, word2idx, idx2word = prepare_vocab(corpus)\n",
    "    logger.info(f\"Vocabulary size: {len(vocab)} words\")\n",
    "    \n",
    "    # Create skipgrams\n",
    "    logger.info(\"Creating skipgrams...\")\n",
    "    all_skipgrams = []\n",
    "    for sentence in tqdm(tokenized, desc=\"Processing sentences\"):\n",
    "        all_skipgrams.extend(create_skipgrams(sentence, window_size))\n",
    "    logger.info(f\"Created {len(all_skipgrams)} skipgrams\")\n",
    "    \n",
    "    # Create unigram table for negative sampling\n",
    "    logger.info(\"Creating unigram table...\")\n",
    "    word_counts = Counter([word for sent in tokenized for word in sent])\n",
    "    unigram_table = create_unigram_table(word_counts, len(vocab))\n",
    "    logger.info(f\"Created unigram table with {len(unigram_table)} entries\")\n",
    "    \n",
    "    # Load evaluation datasets\n",
    "    logger.info(\"Loading evaluation datasets...\")\n",
    "    semantic_pairs, syntactic_pairs = load_word_analogies()\n",
    "    similarities = load_similarity_dataset()\n",
    "    logger.info(f\"Loaded {len(semantic_pairs)} semantic pairs and {len(syntactic_pairs)} syntactic pairs\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SkipgramNeg(len(vocab), embedding_size)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Training metrics\n",
    "    best_loss = float('inf')\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logger.info(f\"\\n{'='*20} Starting Training {'='*20}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        num_batches = len(all_skipgrams) // batch_size + (1 if len(all_skipgrams) % batch_size != 0 else 0)\n",
    "        pbar = tqdm(range(0, len(all_skipgrams), batch_size), \n",
    "                   desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "                   total=num_batches)\n",
    "        \n",
    "        for i in pbar:\n",
    "            # Prepare batch\n",
    "            centers, outsides, negative = prepare_batch(\n",
    "                all_skipgrams[i:i+batch_size],\n",
    "                min(batch_size, len(all_skipgrams) - i),\n",
    "                word2idx,\n",
    "                unigram_table,\n",
    "                neg_samples\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = model(centers, outsides, negative)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            current_loss = loss.item()\n",
    "            epoch_loss += current_loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n",
    "        \n",
    "        # Calculate average loss for epoch\n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Evaluate model\n",
    "        logger.info(f\"\\nEvaluating epoch {epoch+1}...\")\n",
    "        semantic_acc = evaluate_analogies(model, word2idx, idx2word, semantic_pairs)\n",
    "        syntactic_acc = evaluate_analogies(model, word2idx, idx2word, syntactic_pairs)\n",
    "        similarity_corr, mse, num_pairs = evaluate_similarity(model, word2idx, similarities)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        logger.info(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        logger.info(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        logger.info(f\"Semantic Accuracy: {semantic_acc:.4f}\")\n",
    "        logger.info(f\"Syntactic Accuracy: {syntactic_acc:.4f}\")\n",
    "        logger.info(f\"Similarity Correlation: {similarity_corr:.4f}\")\n",
    "        logger.info(f\"MSE: {mse:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            logger.info(\"New best model! Saving checkpoint...\")\n",
    "            model_dir = \"saved_models\"\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model_path = os.path.join(model_dir, f\"w{window_size}_e{embedding_size}_skipgram_neg.pt\")\n",
    "            save_model(model, word2idx, idx2word, model_path, model_type=\"skipgram_neg\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    logger.info(f\"Training Time: {training_time:.2f}s\")\n",
    "    \n",
    "    return model, {\n",
    "        'word2idx': word2idx,\n",
    "        'idx2word': idx2word,\n",
    "        'losses': losses,\n",
    "        'training_time': training_time,\n",
    "        'final_loss': losses[-1] if losses else None,\n",
    "        'best_loss': best_loss,\n",
    "        'semantic_accuracy': semantic_acc,\n",
    "        'syntactic_accuracy': syntactic_acc,\n",
    "        'similarity_correlation': similarity_corr,\n",
    "        'mse': mse,\n",
    "        'num_pairs': num_pairs,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load corpus\n",
    "    corpus = load_news_corpus()\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    # Training configurations\n",
    "    configs = [\n",
    "        {\n",
    "            'window_size': 2,\n",
    "            'embedding_size': 100,\n",
    "            'neg_samples': 5,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 5\n",
    "        },\n",
    "        {\n",
    "            'window_size': 5,\n",
    "            'embedding_size': 100,\n",
    "            'neg_samples': 10,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 5\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    for config in configs:\n",
    "        logger.info(f\"\\nTraining Skip-gram Negative Sampling with config: {config}\")\n",
    "        model, results = train(corpus, **config)\n",
    "        \n",
    "        model_name = f\"Skipgram-NEG (w={config['window_size']}, n={config['neg_samples']})\"\n",
    "        evaluator.evaluate_model(\n",
    "            model, \n",
    "            results['word2idx'], \n",
    "            results['idx2word'],\n",
    "            model_name,\n",
    "            window_size=config['window_size'],\n",
    "            training_time=results['training_time'],\n",
    "            final_loss=results['final_loss']\n",
    "        )\n",
    "    \n",
    "    # Print evaluation results\n",
    "    logger.info(\"\\nTraining Metrics:\")\n",
    "    evaluator.print_training_table()\n",
    "    \n",
    "    logger.info(\"\\nSimilarity Metrics:\")\n",
    "    evaluator.print_similarity_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819eacb5-b9e0-4793-91b9-9b2bec583615",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfc7b43b-0b85-4629-9759-9947f49992e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:\n",
      "Training GloVe with config: {'window_size': 2, 'embedding_size': 100, 'x_max': 100, 'alpha': 0.75, 'batch_size': 128, 'epochs': 5}\n",
      "INFO:__main__:\n",
      "==================== Training Configuration ====================\n",
      "INFO:__main__:Window Size: 2\n",
      "INFO:__main__:Embedding Size: 100\n",
      "INFO:__main__:X_max: 100\n",
      "INFO:__main__:Alpha: 0.75\n",
      "INFO:__main__:Batch Size: 128\n",
      "INFO:__main__:Epochs: 5\n",
      "\n",
      "INFO:__main__:Preparing training data...\n",
      "INFO:__main__:Vocabulary size: 2560 words\n",
      "INFO:__main__:Building co-occurrence matrix...\n",
      "Processing sentences: 100%|██████████| 4623/4623 [00:00<00:00, 12175.00it/s]\n",
      "INFO:__main__:Created co-occurrence matrix with 113821 non-zero entries\n",
      "INFO:__main__:Model parameters: 517,120\n",
      "INFO:__main__:Loading evaluation datasets...\n",
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "INFO:__main__:\n",
      "==================== Starting Training ====================\n",
      "Creating training pairs: 100%|██████████| 113821/113821 [00:00<00:00, 1500276.16it/s]\n",
      "Epoch 1/5: 100%|██████████| 890/890 [00:07<00:00, 126.46it/s, loss=2.0785] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 1...\n",
      "INFO:__main__:\n",
      "Epoch 1 Summary:\n",
      "INFO:__main__:Average Loss: 4.2792\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0586\n",
      "INFO:__main__:MSE: 0.2687\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 113821/113821 [00:00<00:00, 1408358.93it/s]\n",
      "Epoch 2/5: 100%|██████████| 890/890 [00:06<00:00, 129.65it/s, loss=1.3420] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 2...\n",
      "INFO:__main__:\n",
      "Epoch 2 Summary:\n",
      "INFO:__main__:Average Loss: 3.3109\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0478\n",
      "INFO:__main__:MSE: 0.2681\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 113821/113821 [00:00<00:00, 1538203.37it/s]\n",
      "Epoch 3/5: 100%|██████████| 890/890 [00:07<00:00, 121.07it/s, loss=3.6699]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 3...\n",
      "INFO:__main__:\n",
      "Epoch 3 Summary:\n",
      "INFO:__main__:Average Loss: 2.5782\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0376\n",
      "INFO:__main__:MSE: 0.2676\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 113821/113821 [00:00<00:00, 1449116.61it/s]\n",
      "Epoch 4/5: 100%|██████████| 890/890 [00:06<00:00, 131.31it/s, loss=1.3023]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 4...\n",
      "INFO:__main__:\n",
      "Epoch 4 Summary:\n",
      "INFO:__main__:Average Loss: 2.0123\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0340\n",
      "INFO:__main__:MSE: 0.2671\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 113821/113821 [00:00<00:00, 1630085.48it/s]\n",
      "Epoch 5/5: 100%|██████████| 890/890 [00:07<00:00, 124.40it/s, loss=1.2750]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 5...\n",
      "INFO:__main__:\n",
      "Epoch 5 Summary:\n",
      "INFO:__main__:Average Loss: 1.5731\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0380\n",
      "INFO:__main__:MSE: 0.2666\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w2_e100_glove_glove.pt\n",
      "INFO:__main__:\n",
      "==================== Training Complete ====================\n",
      "INFO:__main__:Total training time: 40.77s\n",
      "INFO:__main__:Best loss achieved: 1.5731\n",
      "INFO:__main__:\n",
      "Training GloVe with config: {'window_size': 5, 'embedding_size': 100, 'x_max': 100, 'alpha': 0.75, 'batch_size': 128, 'epochs': 5}\n",
      "INFO:__main__:\n",
      "==================== Training Configuration ====================\n",
      "INFO:__main__:Window Size: 5\n",
      "INFO:__main__:Embedding Size: 100\n",
      "INFO:__main__:X_max: 100\n",
      "INFO:__main__:Alpha: 0.75\n",
      "INFO:__main__:Batch Size: 128\n",
      "INFO:__main__:Epochs: 5\n",
      "\n",
      "INFO:__main__:Preparing training data...\n",
      "INFO:__main__:Vocabulary size: 2560 words\n",
      "INFO:__main__:Building co-occurrence matrix...\n",
      "Processing sentences: 100%|██████████| 4623/4623 [00:00<00:00, 8437.94it/s]\n",
      "INFO:__main__:Created co-occurrence matrix with 221619 non-zero entries\n",
      "INFO:__main__:Model parameters: 517,120\n",
      "INFO:__main__:Loading evaluation datasets...\n",
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "INFO:__main__:\n",
      "==================== Starting Training ====================\n",
      "Creating training pairs: 100%|██████████| 221619/221619 [00:00<00:00, 1568283.74it/s]\n",
      "Epoch 1/5: 100%|██████████| 1732/1732 [00:13<00:00, 129.07it/s, loss=3.9722] \n",
      "INFO:__main__:\n",
      "Evaluating epoch 1...\n",
      "INFO:__main__:\n",
      "Epoch 1 Summary:\n",
      "INFO:__main__:Average Loss: 3.2205\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.1041\n",
      "INFO:__main__:MSE: 0.2877\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 221619/221619 [00:00<00:00, 1518348.42it/s]\n",
      "Epoch 2/5: 100%|██████████| 1732/1732 [00:13<00:00, 130.40it/s, loss=2.1104]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 2...\n",
      "INFO:__main__:\n",
      "Epoch 2 Summary:\n",
      "INFO:__main__:Average Loss: 2.2967\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0873\n",
      "INFO:__main__:MSE: 0.2891\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 221619/221619 [00:00<00:00, 1342988.58it/s]\n",
      "Epoch 3/5: 100%|██████████| 1732/1732 [00:13<00:00, 125.95it/s, loss=2.4845]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 3...\n",
      "INFO:__main__:\n",
      "Epoch 3 Summary:\n",
      "INFO:__main__:Average Loss: 1.6509\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0643\n",
      "INFO:__main__:MSE: 0.2907\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 221619/221619 [00:00<00:00, 1468129.62it/s]\n",
      "Epoch 4/5: 100%|██████████| 1732/1732 [00:13<00:00, 125.34it/s, loss=0.9231]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 4...\n",
      "INFO:__main__:\n",
      "Epoch 4 Summary:\n",
      "INFO:__main__:Average Loss: 1.1849\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0614\n",
      "INFO:__main__:MSE: 0.2919\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 221619/221619 [00:00<00:00, 1626527.09it/s]\n",
      "Epoch 5/5: 100%|██████████| 1732/1732 [00:14<00:00, 121.58it/s, loss=0.8132]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 5...\n",
      "INFO:__main__:\n",
      "Epoch 5 Summary:\n",
      "INFO:__main__:Average Loss: 0.8460\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: 0.0458\n",
      "INFO:__main__:MSE: 0.2929\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w5_e100_glove_glove.pt\n",
      "INFO:__main__:\n",
      "==================== Training Complete ====================\n",
      "INFO:__main__:Total training time: 74.49s\n",
      "INFO:__main__:Best loss achieved: 0.8460\n",
      "INFO:__main__:\n",
      "Training GloVe with config: {'window_size': 10, 'embedding_size': 100, 'x_max': 100, 'alpha': 0.75, 'batch_size': 128, 'epochs': 5}\n",
      "INFO:__main__:\n",
      "==================== Training Configuration ====================\n",
      "INFO:__main__:Window Size: 10\n",
      "INFO:__main__:Embedding Size: 100\n",
      "INFO:__main__:X_max: 100\n",
      "INFO:__main__:Alpha: 0.75\n",
      "INFO:__main__:Batch Size: 128\n",
      "INFO:__main__:Epochs: 5\n",
      "\n",
      "INFO:__main__:Preparing training data...\n",
      "INFO:__main__:Vocabulary size: 2560 words\n",
      "INFO:__main__:Building co-occurrence matrix...\n",
      "Processing sentences: 100%|██████████| 4623/4623 [00:00<00:00, 4875.92it/s]\n",
      "INFO:__main__:Created co-occurrence matrix with 333183 non-zero entries\n",
      "INFO:__main__:Model parameters: 517,120\n",
      "INFO:__main__:Loading evaluation datasets...\n",
      "INFO:__main__:Successfully loaded 203 word pairs from WordSim-353\n",
      "INFO:__main__:Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "INFO:__main__:\n",
      "==================== Starting Training ====================\n",
      "Creating training pairs: 100%|██████████| 333183/333183 [00:00<00:00, 1629485.01it/s]\n",
      "Epoch 1/5: 100%|██████████| 2603/2603 [00:20<00:00, 130.02it/s, loss=2.6360]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 1...\n",
      "INFO:__main__:\n",
      "Epoch 1 Summary:\n",
      "INFO:__main__:Average Loss: 2.4781\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.2737\n",
      "INFO:__main__:MSE: 0.2778\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w10_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 333183/333183 [00:00<00:00, 1377987.43it/s]\n",
      "Epoch 2/5: 100%|██████████| 2603/2603 [00:20<00:00, 128.88it/s, loss=1.2787]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 2...\n",
      "INFO:__main__:\n",
      "Epoch 2 Summary:\n",
      "INFO:__main__:Average Loss: 1.6470\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.2552\n",
      "INFO:__main__:MSE: 0.2779\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w10_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 333183/333183 [00:00<00:00, 1488330.38it/s]\n",
      "Epoch 3/5: 100%|██████████| 2603/2603 [00:20<00:00, 127.64it/s, loss=0.7111]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 3...\n",
      "INFO:__main__:\n",
      "Epoch 3 Summary:\n",
      "INFO:__main__:Average Loss: 1.0999\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.2539\n",
      "INFO:__main__:MSE: 0.2782\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w10_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 333183/333183 [00:00<00:00, 1468347.94it/s]\n",
      "Epoch 4/5: 100%|██████████| 2603/2603 [00:20<00:00, 129.15it/s, loss=0.7844]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 4...\n",
      "INFO:__main__:\n",
      "Epoch 4 Summary:\n",
      "INFO:__main__:Average Loss: 0.7289\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.2519\n",
      "INFO:__main__:MSE: 0.2786\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w10_e100_glove_glove.pt\n",
      "Creating training pairs: 100%|██████████| 333183/333183 [00:00<00:00, 1613433.74it/s]\n",
      "Epoch 5/5: 100%|██████████| 2603/2603 [00:19<00:00, 131.31it/s, loss=0.3477]\n",
      "INFO:__main__:\n",
      "Evaluating epoch 5...\n",
      "INFO:__main__:\n",
      "Epoch 5 Summary:\n",
      "INFO:__main__:Average Loss: 0.4783\n",
      "INFO:__main__:Semantic Accuracy: 0.0000\n",
      "INFO:__main__:Syntactic Accuracy: 0.0000\n",
      "INFO:__main__:Similarity Correlation: -0.2549\n",
      "INFO:__main__:MSE: 0.2792\n",
      "INFO:__main__:New best model! Saving checkpoint...\n",
      "INFO:__main__:Model saved to saved_models/w10_e100_glove_glove.pt\n",
      "INFO:__main__:\n",
      "==================== Training Complete ====================\n",
      "INFO:__main__:Total training time: 107.17s\n",
      "INFO:__main__:Best loss achieved: 0.4783\n",
      "INFO:__main__:\n",
      "Training Metrics:\n",
      "INFO:__main__:\n",
      "Similarity Metrics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Accuracy Results:\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Model                | Window Size     | Training Loss   | Training Time   | Syntactic Acc   | Semantic Acc   \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "GloVe (w=2, α=0.75)  | 2               | 1.5731          | 40.77s          | 0.0000          | 0.0000         \n",
      "GloVe (w=5, α=0.75)  | 5               | 0.8460          | 74.49s          | 0.0000          | 0.0000         \n",
      "GloVe (w=10, α=0.75) | 10              | 0.4783          | 107.17s         | 0.0000          | 0.0000         \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Similarity Comparison Results:\n",
      "---------------------------------------------------\n",
      "Metric          | GloVe           | Y true         \n",
      "---------------------------------------------------\n",
      "MSE             | 0.2666          | 1.0000         \n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_center = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.center_bias = nn.Embedding(voc_size, 1)\n",
    "        self.outside_bias = nn.Embedding(voc_size, 1)\n",
    "        \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embed = self.embedding_center(center)\n",
    "        outside_embed = self.embedding_outside(outside)\n",
    "        \n",
    "        center_bias = self.center_bias(center).squeeze()\n",
    "        outside_bias = self.outside_bias(outside).squeeze()\n",
    "        \n",
    "        inner_product = torch.sum(center_embed * outside_embed, dim=2).squeeze()\n",
    "        \n",
    "        prediction = inner_product + center_bias + outside_bias\n",
    "        \n",
    "        loss = weighting * torch.pow(prediction - torch.log(coocs), 2)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "def build_cooccurrence_matrix(tokenized, vocab_size, word2idx, window_size=5):\n",
    "    \"\"\"Build word co-occurrence matrix\"\"\"\n",
    "    logger.info(\"Building co-occurrence matrix...\")\n",
    "    cooccurrence = defaultdict(float)\n",
    "    \n",
    "    for sentence in tqdm(tokenized, desc=\"Processing sentences\"):\n",
    "        for center_pos, center_word in enumerate(sentence):\n",
    "            center_idx = word2idx.get(center_word, word2idx['<UNK>'])\n",
    "            \n",
    "            # For each context word in window\n",
    "            for context_pos in range(\n",
    "                max(0, center_pos - window_size),\n",
    "                min(len(sentence), center_pos + window_size + 1)\n",
    "            ):\n",
    "                if context_pos != center_pos:\n",
    "                    context_word = sentence[context_pos]\n",
    "                    context_idx = word2idx.get(context_word, word2idx['<UNK>'])\n",
    "                    distance = abs(context_pos - center_pos)\n",
    "                    cooccurrence[(center_idx, context_idx)] += 1.0 / distance\n",
    "    \n",
    "    logger.info(f\"Created co-occurrence matrix with {len(cooccurrence)} non-zero entries\")\n",
    "    return cooccurrence\n",
    "\n",
    "def train(corpus, window_size=5, embedding_size=100, x_max=100, alpha=0.75, batch_size=128, epochs=5):\n",
    "    \"\"\"Train the GloVe model\"\"\"\n",
    "    logger.info(f\"\\n{'='*20} Training Configuration {'='*20}\")\n",
    "    logger.info(f\"Window Size: {window_size}\")\n",
    "    logger.info(f\"Embedding Size: {embedding_size}\")\n",
    "    logger.info(f\"X_max: {x_max}\")\n",
    "    logger.info(f\"Alpha: {alpha}\")\n",
    "    logger.info(f\"Batch Size: {batch_size}\")\n",
    "    logger.info(f\"Epochs: {epochs}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    logger.info(\"Preparing training data...\")\n",
    "    tokenized, vocab, word2idx, idx2word = prepare_vocab(corpus)\n",
    "    logger.info(f\"Vocabulary size: {len(vocab)} words\")\n",
    "    \n",
    "    # Build co-occurrence matrix\n",
    "    cooc_matrix = build_cooccurrence_matrix(tokenized, len(vocab), word2idx, window_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GloVe(len(vocab), embedding_size)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Load evaluation datasets\n",
    "    logger.info(\"Loading evaluation datasets...\")\n",
    "    semantic_pairs, syntactic_pairs = load_word_analogies()\n",
    "    similarities = load_similarity_dataset()\n",
    "    logger.info(f\"Loaded {len(semantic_pairs)} semantic pairs and {len(syntactic_pairs)} syntactic pairs\")\n",
    "    \n",
    "    # Training metrics\n",
    "    best_loss = float('inf')\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logger.info(f\"\\n{'='*20} Starting Training {'='*20}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Create batches from co-occurrence matrix\n",
    "        training_pairs = []\n",
    "        with tqdm(total=len(cooc_matrix), desc=\"Creating training pairs\") as pbar:\n",
    "            for (i, j), xij in cooc_matrix.items():\n",
    "                if xij > 0:\n",
    "                    training_pairs.append((i, j, xij))\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        # Shuffle training pairs\n",
    "        np.random.shuffle(training_pairs)\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        num_batches = len(training_pairs) // batch_size + (1 if len(training_pairs) % batch_size != 0 else 0)\n",
    "        pbar = tqdm(range(0, len(training_pairs), batch_size), \n",
    "                   desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "                   total=num_batches)\n",
    "        \n",
    "        for i in pbar:\n",
    "            # Get batch\n",
    "            batch = training_pairs[i:i + batch_size]\n",
    "            \n",
    "            # Convert to tensors\n",
    "            i_batch = torch.LongTensor([x[0] for x in batch]).unsqueeze(1)\n",
    "            j_batch = torch.LongTensor([x[1] for x in batch]).unsqueeze(1)\n",
    "            xij_batch = torch.FloatTensor([x[2] for x in batch])\n",
    "            \n",
    "            # Weight function\n",
    "            weights = torch.pow(xij_batch / x_max, alpha)\n",
    "            weights[xij_batch > x_max] = 1\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = model(i_batch, j_batch, xij_batch, weights)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n",
    "        \n",
    "        # Calculate average loss for epoch\n",
    "        avg_loss = total_loss / batch_count\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Evaluate model\n",
    "        logger.info(f\"\\nEvaluating epoch {epoch+1}...\")\n",
    "        semantic_acc = evaluate_analogies(model, word2idx, idx2word, semantic_pairs)\n",
    "        syntactic_acc = evaluate_analogies(model, word2idx, idx2word, syntactic_pairs)\n",
    "        similarity_corr, mse, num_pairs = evaluate_similarity(model, word2idx, similarities)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        logger.info(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        logger.info(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        logger.info(f\"Semantic Accuracy: {semantic_acc:.4f}\")\n",
    "        logger.info(f\"Syntactic Accuracy: {syntactic_acc:.4f}\")\n",
    "        logger.info(f\"Similarity Correlation: {similarity_corr:.4f}\")\n",
    "        logger.info(f\"MSE: {mse:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            logger.info(\"New best model! Saving checkpoint...\")\n",
    "            model_dir = \"saved_models\"\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model_path = os.path.join(model_dir, f\"w{window_size}_e{embedding_size}_glove.pt\")\n",
    "            save_model(model, word2idx, idx2word, model_path, model_type=\"glove\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    logger.info(f\"\\n{'='*20} Training Complete {'='*20}\")\n",
    "    logger.info(f\"Total training time: {training_time:.2f}s\")\n",
    "    logger.info(f\"Best loss achieved: {best_loss:.4f}\")\n",
    "    \n",
    "    return model, {\n",
    "        'word2idx': word2idx,\n",
    "        'idx2word': idx2word,\n",
    "        'losses': losses,\n",
    "        'training_time': training_time,\n",
    "        'final_loss': losses[-1] if losses else None,\n",
    "        'best_loss': best_loss,\n",
    "        'semantic_accuracy': semantic_acc,\n",
    "        'syntactic_accuracy': syntactic_acc,\n",
    "        'similarity_correlation': similarity_corr,\n",
    "        'mse': mse,\n",
    "        'num_pairs': num_pairs,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load corpus\n",
    "    corpus = load_news_corpus()\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    # Training configurations\n",
    "    configs = [\n",
    "        {\n",
    "            'window_size': 2,\n",
    "            'embedding_size': 100,\n",
    "            'x_max': 100,\n",
    "            'alpha': 0.75,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 5\n",
    "        },\n",
    "        {\n",
    "            'window_size': 5,\n",
    "            'embedding_size': 100,\n",
    "            'x_max': 100,\n",
    "            'alpha': 0.75,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 5\n",
    "        },\n",
    "        {\n",
    "            'window_size': 10,\n",
    "            'embedding_size': 100,\n",
    "            'x_max': 100,\n",
    "            'alpha': 0.75,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 5\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    for config in configs:\n",
    "        logger.info(f\"\\nTraining GloVe with config: {config}\")\n",
    "        model, results = train(corpus, **config)\n",
    "        \n",
    "        model_name = f\"GloVe (w={config['window_size']}, α={config['alpha']})\"\n",
    "        evaluator.evaluate_model(\n",
    "            model, \n",
    "            results['word2idx'], \n",
    "            results['idx2word'],\n",
    "            model_name,\n",
    "            window_size=config['window_size'],\n",
    "            training_time=results['training_time'],\n",
    "            final_loss=results['final_loss']\n",
    "        )\n",
    "    \n",
    "    # Print evaluation results\n",
    "    logger.info(\"\\nTraining Metrics:\")\n",
    "    evaluator.print_training_table()\n",
    "    \n",
    "    logger.info(\"\\nSimilarity Metrics:\")\n",
    "    evaluator.print_similarity_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3177f3-f537-4414-a8f8-bfb6ac64c07b",
   "metadata": {},
   "source": [
    "# GloVe Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e029f1-7c04-4359-94b8-15696102be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:40:14 - INFO - Successfully loaded 203 word pairs from WordSim-353\n",
      "09:40:14 - INFO - \n",
      "Loading GloVe with config: {'path': 'glove.6B.100d.txt', 'dim': 100}\n",
      "09:40:14 - INFO - \n",
      "==================== Loading Configuration ====================\n",
      "09:40:14 - INFO - Model Path: glove.6B.100d.txt\n",
      "09:40:14 - INFO - Embedding Dimension: 100\n",
      "\n",
      "09:40:14 - INFO - Loading pretrained embeddings...\n",
      "Building vocabulary: 400000it [00:02, 165074.48it/s]\n",
      "Loading embeddings: 400000it [00:10, 38129.74it/s]\n",
      "09:40:27 - INFO - Loaded 400,000 words with dimension 100\n",
      "09:40:27 - INFO - \n",
      "Loading evaluation datasets...\n",
      "09:40:27 - INFO - Successfully loaded 203 word pairs from WordSim-353\n",
      "09:40:27 - INFO - Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "09:40:27 - INFO - \n",
      "==================================================\n",
      "09:40:27 - INFO - Starting Model Evaluation\n",
      "09:40:27 - INFO - ==================================================\n",
      "09:40:27 - INFO - \n",
      "Evaluating semantic analogies...\n",
      "09:45:48 - INFO - Number of semantic pairs evaluated: 12\n",
      "09:45:48 - INFO - Semantic accuracy: 0.9167\n",
      "09:45:48 - INFO - \n",
      "Evaluating syntactic analogies...\n",
      "09:52:32 - INFO - Number of syntactic pairs evaluated: 15\n",
      "09:52:32 - INFO - Syntactic accuracy: 0.5333\n",
      "09:52:32 - INFO - \n",
      "Evaluating word similarities...\n",
      "09:52:32 - INFO - Number of similarity pairs evaluated: 203\n",
      "09:52:32 - INFO - Spearman correlation: 0.6035\n",
      "09:52:32 - INFO - Mean squared error: 0.0502\n",
      "09:52:32 - INFO - \n",
      "Example analogies evaluation:\n",
      "09:52:32 - INFO - Analogy king:man :: queen:woman - Similarity: 0.7581\n",
      "09:52:32 - INFO - Analogy paris:france :: rome:italy - Similarity: 0.7056\n",
      "09:52:32 - INFO - Analogy good:better :: bad:worse - Similarity: 0.5263\n",
      "09:52:32 - INFO - Analogy small:smaller :: large:larger - Similarity: 0.6943\n",
      "09:52:32 - INFO - \n",
      "Example word similarities:\n",
      "09:52:32 - INFO - Similarity between 'man' and 'woman': 0.8323\n",
      "09:52:32 - INFO - Similarity between 'king' and 'queen': 0.7508\n",
      "09:52:32 - INFO - Similarity between 'computer' and 'machine': 0.5942\n",
      "09:52:32 - INFO - Similarity between 'happy' and 'sad': 0.6801\n",
      "09:52:32 - INFO - \n",
      "==================== Evaluation Summary ====================\n",
      "09:52:32 - INFO - Semantic Accuracy: 0.9167 (12 pairs)\n",
      "09:52:32 - INFO - Syntactic Accuracy: 0.5333 (15 pairs)\n",
      "09:52:32 - INFO - Similarity Correlation: 0.6035 (203 pairs)\n",
      "09:52:32 - INFO - Mean Squared Error: 0.0502\n",
      "09:52:32 - INFO - ==================================================\n",
      "09:52:32 - ERROR - Error loading model: 'function' object has no attribute 'embedding_dim'\n",
      "09:52:32 - INFO - \n",
      "Loading GloVe with config: {'path': 'glove.6B.300d.txt', 'dim': 300}\n",
      "09:52:32 - INFO - \n",
      "==================== Loading Configuration ====================\n",
      "09:52:32 - INFO - Model Path: glove.6B.300d.txt\n",
      "09:52:32 - INFO - Embedding Dimension: 300\n",
      "\n",
      "09:52:32 - INFO - Loading pretrained embeddings...\n",
      "Building vocabulary: 400000it [00:06, 64095.88it/s]\n",
      "Loading embeddings: 400000it [00:31, 12855.92it/s]\n",
      "09:53:09 - INFO - Loaded 400,000 words with dimension 300\n",
      "09:53:09 - INFO - \n",
      "Loading evaluation datasets...\n",
      "09:53:09 - INFO - Successfully loaded 203 word pairs from WordSim-353\n",
      "09:53:09 - INFO - Loaded 12 semantic pairs and 15 syntactic pairs\n",
      "09:53:09 - INFO - \n",
      "==================================================\n",
      "09:53:09 - INFO - Starting Model Evaluation\n",
      "09:53:09 - INFO - ==================================================\n",
      "09:53:09 - INFO - \n",
      "Evaluating semantic analogies...\n",
      "09:58:42 - INFO - Number of semantic pairs evaluated: 12\n",
      "09:58:42 - INFO - Semantic accuracy: 0.7500\n",
      "09:58:42 - INFO - \n",
      "Evaluating syntactic analogies...\n",
      "10:05:35 - INFO - Number of syntactic pairs evaluated: 15\n",
      "10:05:35 - INFO - Syntactic accuracy: 0.4000\n",
      "10:05:35 - INFO - \n",
      "Evaluating word similarities...\n",
      "10:05:35 - INFO - Number of similarity pairs evaluated: 203\n",
      "10:05:35 - INFO - Spearman correlation: 0.6638\n",
      "10:05:35 - INFO - Mean squared error: 0.0750\n",
      "10:05:35 - INFO - \n",
      "Example analogies evaluation:\n",
      "10:05:35 - INFO - Analogy king:man :: queen:woman - Similarity: 0.6814\n",
      "10:05:35 - INFO - Analogy paris:france :: rome:italy - Similarity: 0.6228\n",
      "10:05:35 - INFO - Analogy good:better :: bad:worse - Similarity: 0.5290\n",
      "10:05:35 - INFO - Analogy small:smaller :: large:larger - Similarity: 0.6370\n",
      "10:05:35 - INFO - \n",
      "Example word similarities:\n",
      "10:05:35 - INFO - Similarity between 'man' and 'woman': 0.6999\n",
      "10:05:35 - INFO - Similarity between 'king' and 'queen': 0.6336\n",
      "10:05:35 - INFO - Similarity between 'computer' and 'machine': 0.4563\n",
      "10:05:35 - INFO - Similarity between 'happy' and 'sad': 0.5653\n",
      "10:05:35 - INFO - \n",
      "==================== Evaluation Summary ====================\n",
      "10:05:35 - INFO - Semantic Accuracy: 0.7500 (12 pairs)\n",
      "10:05:35 - INFO - Syntactic Accuracy: 0.4000 (15 pairs)\n",
      "10:05:35 - INFO - Similarity Correlation: 0.6638 (203 pairs)\n",
      "10:05:35 - INFO - Mean Squared Error: 0.0750\n",
      "10:05:35 - INFO - ==================================================\n",
      "10:05:35 - ERROR - Error loading model: 'function' object has no attribute 'embedding_dim'\n",
      "10:05:35 - INFO - \n",
      "Training Metrics:\n",
      "10:05:35 - INFO - \n",
      "Similarity Metrics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Accuracy Results:\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Model           | Window Size     | Training Loss   | Training Time   | Syntactic Acc   | Semantic Acc   \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Similarity Comparison Results:\n",
      "---------------------------------\n",
      "Metric          | Y true         \n",
      "---------------------------------\n",
      "MSE             | 1.0000         \n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PretrainedGloVe(nn.Module):\n",
    "    \"\"\"Direct wrapper for pretrained GloVe embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, word2idx):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = {i: word for word, i in word2idx.items()}\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        \n",
    "        # Create embedding layer from pretrained vectors\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings))\n",
    "    \n",
    "    def embedding_center(self, indices):\n",
    "        \"\"\"Match the interface of our other models\"\"\"\n",
    "        return self.embedding(indices)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "def load_pretrained_glove(path, dim=100):\n",
    "    \"\"\"Load pretrained GloVe embeddings directly\n",
    "    \n",
    "    Args:\n",
    "        path: Path to GloVe embeddings file\n",
    "        dim: Embedding dimension\n",
    "        \n",
    "    Returns:\n",
    "        PretrainedGloVe: Model with pretrained embeddings\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n{'='*20} Loading Configuration {'='*20}\")\n",
    "    logger.info(f\"Model Path: {path}\")\n",
    "    logger.info(f\"Embedding Dimension: {dim}\\n\")\n",
    "    \n",
    "    # Load GloVe vectors\n",
    "    logger.info(\"Loading pretrained embeddings...\")\n",
    "    word2idx = {}\n",
    "    vectors = []\n",
    "    \n",
    "    # First pass: collect words and create word2idx\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=\"Building vocabulary\")):\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            word = tokens[0]\n",
    "            word2idx[word] = i\n",
    "    \n",
    "    # Initialize embedding matrix\n",
    "    embeddings = np.zeros((len(word2idx), dim))\n",
    "    \n",
    "    # Second pass: fill embedding matrix\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading embeddings\"):\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            word = tokens[0]\n",
    "            vector = np.array([float(x) for x in tokens[1:]], dtype=np.float32)\n",
    "            embeddings[word2idx[word]] = vector\n",
    "    \n",
    "    logger.info(f\"Loaded {len(word2idx):,} words with dimension {dim}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = PretrainedGloVe(embeddings, word2idx)\n",
    "    \n",
    "    # Load evaluation datasets\n",
    "    logger.info(\"\\nLoading evaluation datasets...\")\n",
    "    semantic_pairs, syntactic_pairs = load_word_analogies()\n",
    "    similarities = load_similarity_dataset()\n",
    "    logger.info(f\"Loaded {len(semantic_pairs)} semantic pairs and {len(syntactic_pairs)} syntactic pairs\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    logger.info(\"\\n\" + \"=\"*50)\n",
    "    logger.info(\"Starting Model Evaluation\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    # Semantic analogies evaluation\n",
    "    logger.info(\"\\nEvaluating semantic analogies...\")\n",
    "    semantic_acc = evaluate_analogies(model, word2idx, model.idx2word, semantic_pairs)\n",
    "    logger.info(f\"Number of semantic pairs evaluated: {len(semantic_pairs)}\")\n",
    "    logger.info(f\"Semantic accuracy: {semantic_acc:.4f}\")\n",
    "    \n",
    "    # Syntactic analogies evaluation\n",
    "    logger.info(\"\\nEvaluating syntactic analogies...\")\n",
    "    syntactic_acc = evaluate_analogies(model, word2idx, model.idx2word, syntactic_pairs)\n",
    "    logger.info(f\"Number of syntactic pairs evaluated: {len(syntactic_pairs)}\")\n",
    "    logger.info(f\"Syntactic accuracy: {syntactic_acc:.4f}\")\n",
    "    \n",
    "    # Word similarity evaluation\n",
    "    logger.info(\"\\nEvaluating word similarities...\")\n",
    "    similarity_corr, mse, num_pairs = evaluate_similarity(model, word2idx, similarities)\n",
    "    logger.info(f\"Number of similarity pairs evaluated: {num_pairs}\")\n",
    "    logger.info(f\"Spearman correlation: {similarity_corr:.4f}\")\n",
    "    logger.info(f\"Mean squared error: {mse:.4f}\")\n",
    "    \n",
    "    # Example analogies\n",
    "    logger.info(\"\\nExample analogies evaluation:\")\n",
    "    example_analogies = [\n",
    "        ('king', 'man', 'queen', 'woman'),\n",
    "        ('paris', 'france', 'rome', 'italy'),\n",
    "        ('good', 'better', 'bad', 'worse'),\n",
    "        ('small', 'smaller', 'large', 'larger')\n",
    "    ]\n",
    "    \n",
    "    for a, b, c, d in example_analogies:\n",
    "        if all(word in word2idx for word in [a, b, c, d]):\n",
    "            # Get embeddings\n",
    "            va = model.embedding(torch.tensor(word2idx[a]))\n",
    "            vb = model.embedding(torch.tensor(word2idx[b]))\n",
    "            vc = model.embedding(torch.tensor(word2idx[c]))\n",
    "            vd = model.embedding(torch.tensor(word2idx[d]))\n",
    "            \n",
    "            # Calculate cosine similarity between analogy pairs\n",
    "            cos = nn.CosineSimilarity(dim=0)\n",
    "            similarity = cos(vb - va, vd - vc)\n",
    "            logger.info(f\"Analogy {a}:{b} :: {c}:{d} - Similarity: {similarity:.4f}\")\n",
    "    \n",
    "    # Example similarities\n",
    "    logger.info(\"\\nExample word similarities:\")\n",
    "    example_pairs = [\n",
    "        ('man', 'woman'),\n",
    "        ('king', 'queen'),\n",
    "        ('computer', 'machine'),\n",
    "        ('happy', 'sad')\n",
    "    ]\n",
    "    \n",
    "    for word1, word2 in example_pairs:\n",
    "        if word1 in word2idx and word2 in word2idx:\n",
    "            # Get embeddings\n",
    "            v1 = model.embedding(torch.tensor(word2idx[word1]))\n",
    "            v2 = model.embedding(torch.tensor(word2idx[word2]))\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            cos = nn.CosineSimilarity(dim=0)\n",
    "            similarity = cos(v1, v2)\n",
    "            logger.info(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
    "    \n",
    "    # Print evaluation summary\n",
    "    logger.info(f\"\\n{'='*20} Evaluation Summary {'='*20}\")\n",
    "    logger.info(f\"Semantic Accuracy: {semantic_acc:.4f} ({len(semantic_pairs)} pairs)\")\n",
    "    logger.info(f\"Syntactic Accuracy: {syntactic_acc:.4f} ({len(syntactic_pairs)} pairs)\")\n",
    "    logger.info(f\"Similarity Correlation: {similarity_corr:.4f} ({num_pairs} pairs)\")\n",
    "    logger.info(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    # Save model in our format\n",
    "    model_dir = \"saved_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f\"glove_pretrained_d{dim}.pt\")\n",
    "    save_model(model, word2idx, model.idx2word, model_path, model_type=\"glove_pretrained\")\n",
    "    logger.info(f\"\\nModel saved to {model_path}\")\n",
    "    \n",
    "    return model, {\n",
    "        'word2idx': word2idx,\n",
    "        'idx2word': model.idx2word,\n",
    "        'semantic_accuracy': semantic_acc,\n",
    "        'syntactic_accuracy': syntactic_acc,\n",
    "        'similarity_correlation': similarity_corr,\n",
    "        'mse': mse,\n",
    "        'num_pairs': num_pairs,\n",
    "        'model_path': model_path,\n",
    "        'vocab_size': len(word2idx),\n",
    "        'embedding_size': dim\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    # Configurations for pretrained models\n",
    "    configs = [\n",
    "        {\n",
    "            'path': 'glove.6B.100d.txt',\n",
    "            'dim': 100\n",
    "        },\n",
    "        {\n",
    "            'path': 'glove.6B.300d.txt',\n",
    "            'dim': 300\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Load and evaluate models\n",
    "    for config in configs:\n",
    "        logger.info(f\"\\nLoading GloVe with config: {config}\")\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            model, results = load_pretrained_glove(**config)\n",
    "            loading_time = time.time() - start_time\n",
    "            \n",
    "            model_name = f\"GloVe-Pretrained (d={config['dim']})\"\n",
    "            evaluator.evaluate_model(\n",
    "                model, \n",
    "                results['word2idx'], \n",
    "                results['idx2word'],\n",
    "                model_name,\n",
    "                window_size=None,  # N/A for pretrained models\n",
    "                training_time=loading_time,  # Use loading time instead\n",
    "                final_loss=None,  # N/A for pretrained models\n",
    "                semantic_accuracy=results['semantic_accuracy'],\n",
    "                syntactic_accuracy=results['syntactic_accuracy'],\n",
    "                similarity_correlation=results['similarity_correlation'],\n",
    "                mse=results['mse']\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"\\nModel Statistics:\")\n",
    "            logger.info(f\"Vocabulary Size: {results['vocab_size']:,}\")\n",
    "            logger.info(f\"Embedding Size: {results['embedding_size']}\")\n",
    "            logger.info(f\"Loading Time: {loading_time:.2f}s\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Pretrained embeddings not found at {config['path']}\")\n",
    "            logger.error(\"Please download the embeddings from https://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "            logger.error(\"and extract them to the 'pretrained' directory\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Print evaluation results\n",
    "    logger.info(\"\\nTraining Metrics:\")\n",
    "    evaluator.print_training_table()\n",
    "    \n",
    "    logger.info(\"\\nSimilarity Metrics:\")\n",
    "    evaluator.print_similarity_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6b674-36fd-44f8-842a-7c91e6d95962",
   "metadata": {},
   "source": [
    "💡 Even though the table values were not printed, the results can be retrieved from the training logs(the results have indeed being logged, so nothing to look here or panic!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce79ef5-3fc7-4f97-8e13-d3463cc3f796",
   "metadata": {},
   "source": [
    "# Task 2: Model Comparison and Analysis\n",
    "1. Compare Skip-gram, Skip-gram negative sampling, GloVe models on training loss, training time. (1 points)✅\n",
    "2) Use Word analogies dataset to calucalte between syntactic and semantic accuracy, similar to the methods in the Word2Vec and GloVe paper. (1 points)✅\n",
    "\n",
    "- Note : using only capital-common-countries for semantic and past-tense for syntactic.\n",
    "- Note : Do not be surprised if you achieve 0% accuracy in these experiments, as this may be due to the limitations of our corpus. If you are curious, you can try the same experiments with a pre-trained GloVe model from the Gensim library for a comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10886800-0e29-4903-bdb2-c324adbdca00",
   "metadata": {},
   "source": [
    "Here's the comparison table: (I even tried experimenting with larger window size since I was getting 0 for Window Size = 2)\n",
    "|Model|Window Size|Training Loss|Training Time|Syntactic Accuracy|Semantic Accuracy|\n",
    "|-----|-----------|-------------|-------------|------------------|-----------------|\n",
    "|Skipgram|2|6.4928|1633.94s|0|0|\n",
    "|Skipgram|5|5.6520|3814.83s|0|0|\n",
    "|Skipgram (NEG)|2|2.7663|155.74s|0|0|\n",
    "|Skipgram (NEG)|5|2.4614|374.31s|0|0|\n",
    "|Glove|2|1.5731|40.77s |0|0|\n",
    "|Glove|5|0.8460|74.49s|0|0|\n",
    "|Glove|10|0.4783|107.17s|0|0|\n",
    "|Glove (Gensim) 6B 100 Dim|2|-|-|0.5333|0.9167|\n",
    "|Glove (Gensim) 6B 300 Dim|2|-|-|0.4000|0.7500|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96f121-7b8f-41d9-b46d-c25f42d40f6e",
   "metadata": {},
   "source": [
    "3. Use the similarity dataset4 to find the correlation between your models’ dot product and the provided similarity metrics. (from scipy.stats import spearmanr) Assess if your embeddings correlate with human judgment. (1 points)✅\n",
    "\n",
    "Here's the comparison table:\n",
    "\n",
    "|Model|Skipgram|NEG|GloVe|GloVe (gensim) 100 Dim|GloVe (gensim) 300 Dim|Y_True|\n",
    "|-----|--------|---|-----|----------------------|----------------------|------|\n",
    "|MSE|0.2474|0.1879|0.2666|0.0750|0.0502|1.0000|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2756e-68cb-43ee-ab94-baa9de204842",
   "metadata": {},
   "source": [
    "## Key Observations & Analysis\n",
    "\n",
    "In terms of **Training Efficiency**: \n",
    "- GloVe demonstrated superior training efficiency with the fastest training times (40-107s) compared to Skip-gram (1633-3814s) and Skip-gram with Negative Sampling (155-374s)\n",
    "- GloVe achieved the lowest training losses (0.4783-1.5731) across all window sizes, showing better convergence than both Skip-gram variants\n",
    "\n",
    "In terms of **Window Size Impact**\n",
    "- Larger window sizes generally improved model performance:\n",
    "  - GloVe's loss decreased from 1.5731 (window=2) to 0.4783 (window=10)\n",
    "  - Skip-gram's loss decreased from 6.4928 (window=2) to 5.6520 (window=5)\n",
    "  - Skip-gram with Negative Sampling's loss decreased from 2.7663 (window=2) to 2.4614 (window=5)\n",
    "\n",
    "In terms of **Accuracy Metrics**\n",
    "- Custom-trained models showed poor performance on semantic and syntactic accuracy (all 0%)\n",
    "- Pre-trained GloVe models performed significantly better:\n",
    "  - 100D model: 91.67% semantic accuracy, 53.33% syntactic accuracy\n",
    "  - 300D model: 75% semantic accuracy, 40% syntactic accuracy\n",
    "\n",
    "In terms of **Mean Squared Error Analysis**\n",
    "- Skip-gram with Negative Sampling achieved the best MSE (0.1879) among custom-trained models\n",
    "- Pre-trained GloVe models significantly outperformed custom models:\n",
    "  - 100D: 0.0502 MSE\n",
    "  - 300D: 0.0750 MSE\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. GloVe's algorithm demonstrates superior computational efficiency while achieving better convergence\n",
    "\n",
    "2. Skip-gram with Negative Sampling shows better performance than basic Skip-gram, suggesting the effectiveness of negative sampling in improving training\n",
    "\n",
    "3. The significant performance gap between custom-trained and pre-trained models highlights the importance of large-scale training data and proper hyperparameter tuning\n",
    "\n",
    "4. The 100D pre-trained GloVe model surprisingly outperformed the 300D model, suggesting that higher dimensionality doesn't always guarantee better performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2aba3-e9fe-4607-9fcb-46d2428c98bd",
   "metadata": {},
   "source": [
    "# Similar 10 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d36265ad-9768-424a-a6d7-ba2365318ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading Skip-gram from: /home/jupyter-st125462/NLP/A1/saved_models/w2_e100_skipgram.pt\n",
      "/tmp/ipykernel_762838/3318003762.py:379: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n",
      "INFO:__main__:Successfully loaded Skip-gram\n",
      "INFO:__main__:Loading Skip-gram (Neg) from: /home/jupyter-st125462/NLP/A1/saved_models/w2_e100_skipgram_neg_skipgram_neg.pt\n",
      "INFO:__main__:Successfully loaded Skip-gram (Neg)\n",
      "INFO:__main__:Loading GloVe from: /home/jupyter-st125462/NLP/A1/saved_models/w2_e100_glove_glove.pt\n",
      "INFO:__main__:Successfully loaded GloVe\n",
      "WARNING:__main__:Word 'king' not in vocabulary for Skip-gram\n",
      "WARNING:__main__:Word 'king' not in vocabulary for Skip-gram (Neg)\n",
      "WARNING:__main__:Word 'king' not in vocabulary for GloVe\n",
      "WARNING:__main__:Word 'computer' not in vocabulary for Skip-gram\n",
      "WARNING:__main__:Word 'computer' not in vocabulary for Skip-gram (Neg)\n",
      "WARNING:__main__:Word 'computer' not in vocabulary for GloVe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar words to 'king':\n",
      "================================================================================\n",
      "No results found for 'king'\n",
      "\n",
      "\n",
      "Similar words to 'computer':\n",
      "================================================================================\n",
      "No results found for 'computer'\n",
      "\n",
      "\n",
      "Similar words to 'good':\n",
      "================================================================================\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe        |    Sim |\n",
      "+========+=============+========+===================+========+==============+========+\n",
      "|      1 | brevard     | 0.3579 | size              | 0.4519 | pitchers     | 0.3564 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|      2 | nomination  | 0.3514 | give              | 0.4355 | four         | 0.3358 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|      3 | setting     | 0.3222 | property          | 0.4291 | manufacturer | 0.3077 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|      4 | important   | 0.3195 | parker            | 0.4269 | sue          | 0.2992 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|      5 | c.          | 0.3181 | 1953              | 0.4258 | italian      | 0.2947 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|      6 | group       | 0.3123 | women's           | 0.4098 | present      | 0.2939 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|      7 | increased   | 0.3071 | the               | 0.393  | eisenhower   | 0.2796 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|      8 | table       | 0.3063 | arms              | 0.3891 | hot          | 0.2754 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "|      9 | clark       | 0.3012 | 15                | 0.3887 | corp.        | 0.2728 |\n",
      "+--------+-------------+--------+-------------------+--------+--------------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'day':\n",
      "================================================================================\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe     |    Sim |\n",
      "+========+=============+========+===================+========+===========+========+\n",
      "|      1 | christmas   | 0.3743 | reason            | 0.404  | hawksley  | 0.4003 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|      2 | problems    | 0.3686 | hill              | 0.3953 | although  | 0.3108 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|      3 | calls       | 0.348  | harry             | 0.3843 | want      | 0.3097 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|      4 | address     | 0.3348 | 15                | 0.3777 | reading   | 0.2942 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|      5 | order       | 0.3309 | city's            | 0.3717 | schedule  | 0.2757 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|      6 | immediate   | 0.3283 | younger           | 0.3668 | efforts   | 0.2709 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|      7 | p.m.        | 0.3258 | an                | 0.3653 | treatment | 0.2697 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|      8 | opportunity | 0.3223 | johnston          | 0.3611 | learned   | 0.2649 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "|      9 | informed    | 0.3212 | additional        | 0.3603 | april     | 0.2623 |\n",
      "+--------+-------------+--------+-------------------+--------+-----------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'time':\n",
      "================================================================================\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe       |    Sim |\n",
      "+========+=============+========+===================+========+=============+========+\n",
      "|      1 | not         | 0.5119 | audience          | 0.4409 | higher      | 0.3364 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      2 | victory     | 0.398  | great             | 0.4259 | competition | 0.2999 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      3 | will        | 0.3484 | responsibility    | 0.4087 | offer       | 0.288  |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      4 | about       | 0.3436 | <UNK>             | 0.404  | treatment   | 0.2824 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      5 | large       | 0.3395 | mr.               | 0.4013 | lake        | 0.2786 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      6 | homes       | 0.3289 | doesn't           | 0.3965 | la          | 0.2726 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      7 | raising     | 0.3279 | that              | 0.3945 | neutral     | 0.2718 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      8 | proposed    | 0.3273 | all               | 0.3937 | stopped     | 0.2713 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      9 | previous    | 0.3266 | thought           | 0.3856 | added       | 0.2696 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'person':\n",
      "================================================================================\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe       |    Sim |\n",
      "+========+=============+========+===================+========+=============+========+\n",
      "|      1 | 1959        | 0.4341 | did               | 0.4501 | individuals | 0.3723 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      2 | comes       | 0.3612 | baltimore         | 0.441  | supreme     | 0.3623 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      3 | nine        | 0.3565 | indeed            | 0.3679 | mills       | 0.3576 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      4 | produced    | 0.3518 | trouble           | 0.3671 | library     | 0.332  |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      5 | cost        | 0.3374 | pirates           | 0.3642 | 1949        | 0.3107 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      6 | personnel   | 0.3316 | belgians          | 0.3469 | first       | 0.2889 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      7 | over        | 0.3164 | reply             | 0.3351 | mayor       | 0.2843 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      8 | board       | 0.3083 | agreed            | 0.329  | half        | 0.2801 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "|      9 | only        | 0.308  | a.m.              | 0.3223 | paso        | 0.2785 |\n",
      "+--------+-------------+--------+-------------------+--------+-------------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'world':\n",
      "================================================================================\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe   |    Sim |\n",
      "+========+=============+========+===================+========+=========+========+\n",
      "|      1 | opinion     | 0.4198 | stage             | 0.4094 | ap      | 0.3047 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      2 | marr        | 0.372  | they're           | 0.4077 | mary    | 0.3027 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      3 | he          | 0.3483 | final             | 0.4073 | rayburn | 0.2853 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      4 | long        | 0.3454 | military          | 0.3936 | holmes  | 0.2788 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      5 | entering    | 0.3378 | car               | 0.3927 | walter  | 0.2762 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      6 | scene       | 0.3341 | order             | 0.3844 | coast   | 0.2678 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      7 | last        | 0.3122 | better            | 0.3832 | shea    | 0.2648 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      8 | very        | 0.3119 | giants            | 0.3798 | vice    | 0.2632 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      9 | i           | 0.3063 | all               | 0.3774 | back    | 0.253  |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'work':\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Word 'data' not in vocabulary for Skip-gram\n",
      "WARNING:__main__:Word 'data' not in vocabulary for Skip-gram (Neg)\n",
      "WARNING:__main__:Word 'data' not in vocabulary for GloVe\n",
      "WARNING:__main__:Word 'algorithm' not in vocabulary for Skip-gram\n",
      "WARNING:__main__:Word 'algorithm' not in vocabulary for Skip-gram (Neg)\n",
      "WARNING:__main__:Word 'algorithm' not in vocabulary for GloVe\n",
      "WARNING:__main__:Word 'network' not in vocabulary for Skip-gram\n",
      "WARNING:__main__:Word 'network' not in vocabulary for Skip-gram (Neg)\n",
      "WARNING:__main__:Word 'network' not in vocabulary for GloVe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe         |    Sim |\n",
      "+========+=============+========+===================+========+===============+========+\n",
      "|      1 | income      | 0.3792 | ,                 | 0.4035 | parker        | 0.3405 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      2 | for         | 0.3658 | states            | 0.4029 | ramsey        | 0.2976 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      3 | earnings    | 0.356  | soviet            | 0.3916 | witnesses     | 0.2955 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      4 | posts       | 0.3471 | taken             | 0.3847 | corn          | 0.2861 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      5 | bob         | 0.3403 | grady             | 0.3801 | senators      | 0.2822 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      6 | post        | 0.3355 | music             | 0.3787 | serve         | 0.2753 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      7 | camp        | 0.3307 | england           | 0.3783 | massachusetts | 0.2701 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      8 | orders      | 0.3253 | change            | 0.3755 | budget        | 0.268  |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      9 | out         | 0.3233 | fact              | 0.3721 | points        | 0.2663 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'data':\n",
      "================================================================================\n",
      "No results found for 'data'\n",
      "\n",
      "\n",
      "Similar words to 'algorithm':\n",
      "================================================================================\n",
      "No results found for 'algorithm'\n",
      "\n",
      "\n",
      "Similar words to 'network':\n",
      "================================================================================\n",
      "No results found for 'network'\n",
      "\n",
      "\n",
      "Similar words to 'science':\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Word 'python' not in vocabulary for Skip-gram\n",
      "WARNING:__main__:Word 'python' not in vocabulary for Skip-gram (Neg)\n",
      "WARNING:__main__:Word 'python' not in vocabulary for GloVe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe         |    Sim |\n",
      "+========+=============+========+===================+========+===============+========+\n",
      "|      1 | works       | 0.3314 | urged             | 0.4003 | doesn't       | 0.3474 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      2 | congolese   | 0.32   | benington         | 0.3727 | eliminate     | 0.3293 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      3 | sheriff     | 0.3009 | how               | 0.3685 | recovery      | 0.2973 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      4 | weather     | 0.2996 | lao               | 0.3521 | none          | 0.2962 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      5 | stressed    | 0.289  | do                | 0.348  | gubernatorial | 0.2871 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      6 | right       | 0.2847 | proposed          | 0.3459 | ben           | 0.2855 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      7 | driven      | 0.2812 | rules             | 0.3355 | women         | 0.2854 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      8 | declared    | 0.2792 | gordon            | 0.3289 | wanted        | 0.2667 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "|      9 | leader      | 0.278  | frank             | 0.324  | must          | 0.2643 |\n",
      "+--------+-------------+--------+-------------------+--------+---------------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'python':\n",
      "================================================================================\n",
      "No results found for 'python'\n",
      "\n",
      "\n",
      "Similar words to 'machine':\n",
      "================================================================================\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe    |    Sim |\n",
      "+========+=============+========+===================+========+==========+========+\n",
      "|      1 | hope        | 0.3684 | contributions     | 0.3766 | below    | 0.3292 |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|      2 | threat      | 0.3361 | while             | 0.3745 | problem  | 0.3264 |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|      3 | people      | 0.3242 | attend            | 0.3589 | remains  | 0.3135 |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|      4 | boston      | 0.3204 | de                | 0.3551 | stock    | 0.3078 |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|      5 | recommended | 0.3133 | christ            | 0.3494 | change   | 0.2955 |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|      6 | yesterday   | 0.3086 | states            | 0.3268 | sports   | 0.294  |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|      7 | under       | 0.3083 | entertainment     | 0.3264 | champion | 0.2919 |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|      8 | kitchen     | 0.2988 | areas             | 0.3184 | senators | 0.2806 |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "|      9 | business    | 0.2967 | district          | 0.3179 | shares   | 0.2799 |\n",
      "+--------+-------------+--------+-------------------+--------+----------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'learning':\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Word 'artificial' not in vocabulary for Skip-gram\n",
      "WARNING:__main__:Word 'artificial' not in vocabulary for Skip-gram (Neg)\n",
      "WARNING:__main__:Word 'artificial' not in vocabulary for GloVe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|   Rank | Skip-gram   |    Sim | Skip-gram (Neg)   |    Sim | GloVe   |    Sim |\n",
      "+========+=============+========+===================+========+=========+========+\n",
      "|      1 | rose        | 0.3376 | harris            | 0.3955 | found   | 0.4245 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      2 | criminal    | 0.3365 | been              | 0.3683 | mills   | 0.3497 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      3 | wisdom      | 0.3349 | pitching          | 0.3565 | ruling  | 0.3168 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      4 | tshombe     | 0.3134 | harvey            | 0.3411 | b.      | 0.3119 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      5 | time        | 0.3044 | ap                | 0.3361 | why     | 0.3105 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      6 | going       | 0.3033 | executive         | 0.335  | officer | 0.3087 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      7 | each        | 0.2848 | struck            | 0.3273 | advance | 0.2924 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      8 | joan        | 0.2836 | gin               | 0.3244 | john    | 0.291  |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "|      9 | furniture   | 0.2828 | senate            | 0.3219 | adopted | 0.2843 |\n",
      "+--------+-------------+--------+-------------------+--------+---------+--------+\n",
      "\n",
      "\n",
      "Similar words to 'artificial':\n",
      "================================================================================\n",
      "No results found for 'artificial'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load all available models\"\"\"\n",
    "    models = {}\n",
    "    base_path = \"/home/jupyter-st125462/NLP/A1/saved_models\"\n",
    "    \n",
    "    # Model configurations\n",
    "    model_configs = {\n",
    "        'skipgram': {\n",
    "            'class': Skipgram,\n",
    "            'path': f\"{base_path}/w2_e100_skipgram.pt\",\n",
    "            'name': \"Skip-gram\"\n",
    "        },\n",
    "        'skipgram_neg': {\n",
    "            'class': SkipgramNeg,\n",
    "            'path': f\"{base_path}/w2_e100_skipgram_neg_skipgram_neg.pt\",\n",
    "            'name': \"Skip-gram (Neg)\"\n",
    "        },\n",
    "        'glove': {\n",
    "            'class': GloVe,\n",
    "            'path': f\"{base_path}/w2_e100_glove_glove.pt\",\n",
    "            'name': \"GloVe\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Try loading each model\n",
    "    for model_type, config in model_configs.items():\n",
    "        try:\n",
    "            logger.info(f\"Loading {config['name']} from: {config['path']}\")\n",
    "            model, word2idx, idx2word = load_model(config['class'], config['path'])\n",
    "            model.eval()\n",
    "            models[model_type] = {\n",
    "                'model': model,\n",
    "                'word2idx': word2idx,\n",
    "                'idx2word': idx2word,\n",
    "                'name': config['name']\n",
    "            }\n",
    "            logger.info(f\"Successfully loaded {config['name']}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load {config['name']}: {str(e)}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def display_similar_words_comparison(query_words, models, top_k=10):\n",
    "    \"\"\"Display similar words comparison across all models\"\"\"\n",
    "    for query in query_words:\n",
    "        print(f\"\\nSimilar words to '{query}':\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Collect results from all models\n",
    "        all_results = []\n",
    "        headers = [\"Rank\"]\n",
    "        \n",
    "        # Add model names to headers\n",
    "        for model_info in models.values():\n",
    "            headers.append(f\"{model_info['name']}\")\n",
    "            headers.append(\"Sim\")\n",
    "        \n",
    "        # Get similar words from each model\n",
    "        max_rows = 0\n",
    "        model_results = {}\n",
    "        \n",
    "        for model_type, model_info in models.items():\n",
    "            if query not in model_info['word2idx']:\n",
    "                logger.warning(f\"Word '{query}' not in vocabulary for {model_info['name']}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                similar = find_similar_words(\n",
    "                    query, \n",
    "                    model_info['model'],\n",
    "                    model_info['word2idx'],\n",
    "                    model_info['idx2word'],\n",
    "                    top_k\n",
    "                )\n",
    "                model_results[model_type] = [\n",
    "                    (word, sim) for word, sim in similar if word != query\n",
    "                ]\n",
    "                max_rows = max(max_rows, len(model_results[model_type]))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error finding similar words for {model_info['name']}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Create table rows\n",
    "        table_data = []\n",
    "        for i in range(max_rows):\n",
    "            row = [f\"{i+1}\"]\n",
    "            for model_type, model_info in models.items():\n",
    "                if model_type in model_results and i < len(model_results[model_type]):\n",
    "                    word, sim = model_results[model_type][i]\n",
    "                    row.extend([word, f\"{sim:.4f}\"])\n",
    "                else:\n",
    "                    row.extend([\"\", \"\"])\n",
    "            table_data.append(row)\n",
    "        \n",
    "        if table_data:\n",
    "            print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "        else:\n",
    "            print(f\"No results found for '{query}'\")\n",
    "        print()\n",
    "\n",
    "def main():\n",
    "    # Load all available models\n",
    "    models = load_models()\n",
    "    \n",
    "    if not models:\n",
    "        logger.error(\"No models could be loaded. Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    # Query words to test\n",
    "    query_words = [\n",
    "        # Common words\n",
    "        \"good\", \"day\", \"time\", \"person\", \"world\", \"work\",\n",
    "        \"news\", \"sad\", \"lion\", \"donkey\",\n",
    "        \"man\", \"woman\", \"learning\", \"language\"\n",
    "    ]\n",
    "    \n",
    "    # Display similar words comparison\n",
    "    display_similar_words_comparison(query_words, models)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69740a5c-822c-4538-a2b4-34c13734a7d9",
   "metadata": {},
   "source": [
    "## Fun Analysis of Top 10 Words predicted by various models 🤩\n",
    "\n",
    "I. Common Words Performance\n",
    "\n",
    "- For common words like \"good\", \"day\", \"time\", and \"person\", all models found related words but with varying degrees of semantic relevance\n",
    "Skip-gram with Negative Sampling (NEG) generally produced higher similarity scores (0.40-0.45) compared to basic Skip-gram (0.30-0.35) and GloVe (0.25-0.35)\n",
    "- For the word \"time\", Skip-gram captured temporal relationships (\"previous\", \"last\") while Skip-gram NEG focused more on contextual usage (\"audience\", \"responsibility\")\n",
    "GloVe showed better performance in capturing related concepts, like \"individuals\" for \"person\" and \"competition\" for \"time\"\n",
    "- Technical terms like \"data\", \"algorithm\", \"network\", \"python\", and \"artificial\" were not in the vocabulary, indicating limitations of the training corpus\n",
    "\n",
    "This proves that training data was likely news-focused (which is the Brown corpus in ouyr case) rather than technical or scientific text.\n",
    "\n",
    "II. Model-Specific Insights\n",
    "\n",
    "1. Skip-gram Model\n",
    "- Tends to find grammatically similar words\n",
    "- Shows lower similarity scores overall (mostly 0.30-0.35)\n",
    "- Often captures syntactic relationships better than semantic ones\n",
    "\n",
    "2. Skip-gram with Negative Sampling\n",
    "- Produces higher similarity scores (0.35-0.45)\n",
    "- Shows better performance in capturing contextual relationships\n",
    "- More computationally efficient while maintaining good quality of word relationships\n",
    "\n",
    "3. GloVe Model\n",
    "- More balanced between syntactic and semantic relationships\n",
    "- Generally produces moderate similarity scores (0.25-0.35)\n",
    "- Shows better performance in capturing domain-specific relationships\n",
    "\n",
    "III. Limitations and Observations\n",
    "\n",
    "- The absence of technical terms suggests a domain-specific bias in the training corpus\n",
    "- All models struggle with rare words or domain-specific terminology\n",
    "- The similarity scores vary significantly across models, indicating different approaches to capturing word relationships\n",
    "- The vocabulary size appears limited, which affects the models' ability to represent a broad range of concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc6c43-2918-46db-b2dc-26437d4d5863",
   "metadata": {},
   "source": [
    "# Thank You : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571dd4f9-5f5c-474b-9fd6-64d118699a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
